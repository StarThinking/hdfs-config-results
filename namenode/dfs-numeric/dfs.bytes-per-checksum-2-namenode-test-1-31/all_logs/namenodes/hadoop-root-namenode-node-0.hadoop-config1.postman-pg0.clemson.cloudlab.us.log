2019-03-25 21:19:54,988 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-25 21:19:55,000 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-25 21:19:55,005 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-25 21:19:55,251 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-25 21:19:55,296 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-25 21:19:55,403 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-25 21:19:55,403 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-25 21:19:55,454 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-25 21:19:55,455 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-25 21:19:55,620 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-25 21:19:55,647 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-25 21:19:55,665 INFO  util.log Log.java:initialized:192 - Logging initialized @1194ms
2019-03-25 21:19:55,774 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-25 21:19:55,787 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-25 21:19:55,797 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-25 21:19:55,800 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-25 21:19:55,801 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-25 21:19:55,802 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-25 21:19:55,828 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-25 21:19:55,828 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-25 21:19:55,837 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-25 21:19:55,838 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-25 21:19:55,875 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-25 21:19:55,875 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-25 21:19:55,951 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-25 21:19:55,971 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-25 21:19:55,971 INFO  server.Server Server.java:doStart:414 - Started @1502ms
2019-03-25 21:19:56,324 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-25 21:19:56,381 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-25 21:19:56,396 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-25 21:19:56,398 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-25 21:19:56,400 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-25 21:19:56,407 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-25 21:19:56,407 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-25 21:19:56,407 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-25 21:19:56,408 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-25 21:19:56,408 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-25 21:19:56,451 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-25 21:19:56,463 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-25 21:19:56,464 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-25 21:19:56,468 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-25 21:19:56,469 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 25 21:19:56
2019-03-25 21:19:56,471 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-25 21:19:56,471 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:19:56,473 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-25 21:19:56,473 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-25 21:19:56,610 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-25 21:19:56,619 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-25 21:19:56,620 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-25 21:19:56,620 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-25 21:19:56,620 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-25 21:19:56,620 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-25 21:19:56,620 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-25 21:19:56,620 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-25 21:19:56,620 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-25 21:19:56,621 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-25 21:19:56,621 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = false
2019-03-25 21:19:56,621 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-25 21:19:56,705 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-25 21:19:56,705 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:19:56,706 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-25 21:19:56,706 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-25 21:19:56,779 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-25 21:19:56,779 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-25 21:19:56,779 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-25 21:19:56,779 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-25 21:19:56,787 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-25 21:19:56,789 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-25 21:19:56,795 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-25 21:19:56,795 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:19:56,795 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-25 21:19:56,795 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-25 21:19:56,823 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-25 21:19:56,824 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-25 21:19:56,824 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-25 21:19:56,828 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-25 21:19:56,828 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-25 21:19:56,831 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-25 21:19:56,831 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:19:56,831 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-25 21:19:56,831 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-25 21:19:56,898 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 83445@clnode059.clemson.cloudlab.us
2019-03-25 21:19:58,081 INFO  namenode.FSImage FSImage.java:loadFSImage:718 - No edit log streams selected.
2019-03-25 21:19:58,081 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-25 21:19:58,144 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-25 21:19:58,147 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-25 21:19:58,178 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-25 21:19:58,178 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-25 21:19:58,183 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-25 21:19:58,183 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 0 entries 0 lookups
2019-03-25 21:19:58,183 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 1344 msecs
2019-03-25 21:19:58,364 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-25 21:19:58,369 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-25 21:19:58,382 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-25 21:19:58,567 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-25 21:19:58,576 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 0
2019-03-25 21:19:58,586 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 0 secs
2019-03-25 21:19:58,587 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 0 racks and 0 datanodes
2019-03-25 21:19:58,587 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-25 21:19:58,621 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-25 21:19:58,622 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-25 21:19:58,624 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-25 21:19:58,627 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-25 21:19:58,632 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-25 21:19:58,637 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-25 21:20:04,477 INFO  namenode.TransferFsImage TransferFsImage.java:copyFileToStream:396 - Sending fileName: /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, fileSize: 389. Sent total: 389 bytes. Size of last segment intended to send: -1 bytes.
2019-03-25 21:20:04,641 ERROR namenode.NameNode LogAdapter.java:error:75 - RECEIVED SIGNAL 15: SIGTERM
2019-03-25 21:20:04,644 INFO  namenode.NameNode LogAdapter.java:info:51 - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode059.clemson.cloudlab.us/130.127.133.68
************************************************************/
2019-03-25 21:20:20,666 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-25 21:20:20,678 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-25 21:20:20,683 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-25 21:20:20,927 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-25 21:20:20,971 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-25 21:20:21,075 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-25 21:20:21,075 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-25 21:20:21,127 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-25 21:20:21,128 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-25 21:20:21,291 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-25 21:20:21,318 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-25 21:20:21,336 INFO  util.log Log.java:initialized:192 - Logging initialized @1182ms
2019-03-25 21:20:21,445 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-25 21:20:21,458 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-25 21:20:21,469 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-25 21:20:21,472 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-25 21:20:21,473 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-25 21:20:21,473 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-25 21:20:21,500 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-25 21:20:21,500 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-25 21:20:21,509 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-25 21:20:21,511 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-25 21:20:21,546 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-25 21:20:21,547 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-25 21:20:21,621 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-25 21:20:21,647 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-25 21:20:21,648 INFO  server.Server Server.java:doStart:414 - Started @1495ms
2019-03-25 21:20:21,979 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-25 21:20:22,037 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-25 21:20:22,056 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-25 21:20:22,058 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-25 21:20:22,062 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-25 21:20:22,072 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-25 21:20:22,072 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-25 21:20:22,072 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-25 21:20:22,073 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-25 21:20:22,073 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-25 21:20:22,127 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-25 21:20:22,139 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-25 21:20:22,139 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-25 21:20:22,144 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-25 21:20:22,144 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 25 21:20:22
2019-03-25 21:20:22,147 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-25 21:20:22,147 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:20:22,148 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-25 21:20:22,149 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-25 21:20:22,281 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-25 21:20:22,290 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-25 21:20:22,290 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-25 21:20:22,290 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-25 21:20:22,290 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-25 21:20:22,291 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-25 21:20:22,291 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-25 21:20:22,291 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-25 21:20:22,291 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-25 21:20:22,291 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-25 21:20:22,291 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = false
2019-03-25 21:20:22,291 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-25 21:20:22,375 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-25 21:20:22,376 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:20:22,376 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-25 21:20:22,376 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-25 21:20:22,449 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-25 21:20:22,449 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-25 21:20:22,450 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-25 21:20:22,450 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-25 21:20:22,457 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-25 21:20:22,460 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-25 21:20:22,465 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-25 21:20:22,465 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:20:22,466 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-25 21:20:22,466 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-25 21:20:22,494 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-25 21:20:22,494 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-25 21:20:22,494 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-25 21:20:22,498 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-25 21:20:22,498 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-25 21:20:22,501 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-25 21:20:22,501 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:20:22,501 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-25 21:20:22,501 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-25 21:20:22,558 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 84125@clnode059.clemson.cloudlab.us
2019-03-25 21:20:23,926 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 1131ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=344ms
GC pool 'PS Scavenge' had collection(s): count=1 time=900ms
2019-03-25 21:20:25,045 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:25,045 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:25,045 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:26,046 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:26,046 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:26,046 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:27,047 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:27,047 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:27,047 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:28,048 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:28,048 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:28,048 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:28,664 INFO  client.QuorumJournalManager QuorumCall.java:waitFor:187 - Waited 6001 ms (timeout=20000 ms) for a response for selectInputStreams. No responses yet.
2019-03-25 21:20:29,049 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:29,049 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:29,049 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-25 21:20:29,642 INFO  namenode.FSImage FSImage.java:loadFSImage:718 - No edit log streams selected.
2019-03-25 21:20:29,642 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-25 21:20:29,706 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-25 21:20:29,708 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-25 21:20:29,739 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-25 21:20:29,740 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-25 21:20:29,745 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-25 21:20:29,745 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 0 entries 0 lookups
2019-03-25 21:20:29,745 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 7236 msecs
2019-03-25 21:20:29,928 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-25 21:20:29,933 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-25 21:20:29,946 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-25 21:20:30,134 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-25 21:20:30,143 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 0
2019-03-25 21:20:30,153 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 0 secs
2019-03-25 21:20:30,153 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 0 racks and 0 datanodes
2019-03-25 21:20:30,153 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-25 21:20:30,188 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-25 21:20:30,188 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-25 21:20:30,191 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-25 21:20:30,195 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-25 21:20:30,201 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-25 21:20:30,207 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-25 21:20:30,691 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.6:9866, datanodeUuid=2ff9f276-e63a-4764-becf-845c28ea53d9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage 2ff9f276-e63a-4764-becf-845c28ea53d9
2019-03-25 21:20:30,694 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.6:9866
2019-03-25 21:20:30,695 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 2ff9f276-e63a-4764-becf-845c28ea53d9 (10.10.1.6:9866).
2019-03-25 21:20:30,698 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=07af74c7-66ec-4346-ac5d-4cb0b1813108, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage 07af74c7-66ec-4346-ac5d-4cb0b1813108
2019-03-25 21:20:30,698 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.3:9866
2019-03-25 21:20:30,699 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 07af74c7-66ec-4346-ac5d-4cb0b1813108 (10.10.1.3:9866).
2019-03-25 21:20:30,761 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e
2019-03-25 21:20:30,761 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.2:9866
2019-03-25 21:20:30,762 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e (10.10.1.2:9866).
2019-03-25 21:20:30,786 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-44cb71b1-9161-4abf-b5a2-a65184867edd for DN 10.10.1.3:9866
2019-03-25 21:20:30,790 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-0db6f4a6-f885-44e0-baa4-46dcc7f80d5b for DN 10.10.1.6:9866
2019-03-25 21:20:30,811 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-ceafbb3d-e9f8-4658-a7a8-e99b7369ec97 for DN 10.10.1.2:9866
2019-03-25 21:20:30,837 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xfd7e4e9b601ea2c2: Processing first storage report for DS-44cb71b1-9161-4abf-b5a2-a65184867edd from datanode 07af74c7-66ec-4346-ac5d-4cb0b1813108
2019-03-25 21:20:30,839 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xfd7e4e9b601ea2c2: from storage DS-44cb71b1-9161-4abf-b5a2-a65184867edd node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=07af74c7-66ec-4346-ac5d-4cb0b1813108, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-03-25 21:20:30,839 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xcce998643b868ac2: Processing first storage report for DS-ceafbb3d-e9f8-4658-a7a8-e99b7369ec97 from datanode dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e
2019-03-25 21:20:30,840 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xcce998643b868ac2: from storage DS-ceafbb3d-e9f8-4658-a7a8-e99b7369ec97 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-25 21:20:30,840 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xbfe0a867b0ef729c: Processing first storage report for DS-0db6f4a6-f885-44e0-baa4-46dcc7f80d5b from datanode 2ff9f276-e63a-4764-becf-845c28ea53d9
2019-03-25 21:20:30,840 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xbfe0a867b0ef729c: from storage DS-0db6f4a6-f885-44e0-baa4-46dcc7f80d5b node DatanodeRegistration(10.10.1.6:9866, datanodeUuid=2ff9f276-e63a-4764-becf-845c28ea53d9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-25 21:20:31,189 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=91a1ec7d-fa15-4ec2-ad49-89860631c88c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage 91a1ec7d-fa15-4ec2-ad49-89860631c88c
2019-03-25 21:20:31,190 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.5:9866
2019-03-25 21:20:31,190 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 91a1ec7d-fa15-4ec2-ad49-89860631c88c (10.10.1.5:9866).
2019-03-25 21:20:31,232 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-880c1f78-6656-49c2-94e1-a38e8b6d5078 for DN 10.10.1.5:9866
2019-03-25 21:20:31,255 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xfc91d2945a263c09: Processing first storage report for DS-880c1f78-6656-49c2-94e1-a38e8b6d5078 from datanode 91a1ec7d-fa15-4ec2-ad49-89860631c88c
2019-03-25 21:20:31,255 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xfc91d2945a263c09: from storage DS-880c1f78-6656-49c2-94e1-a38e8b6d5078 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=91a1ec7d-fa15-4ec2-ad49-89860631c88c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2019-03-25 21:20:31,544 INFO  namenode.FSNamesystem FSNamesystem.java:stopStandbyServices:1408 - Stopping services started for standby state
2019-03-25 21:20:31,545 WARN  ha.EditLogTailer EditLogTailer.java:doWork:481 - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-03-25 21:20:31,649 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1203 - Starting services required for active state
2019-03-25 21:20:31,659 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:435 - Starting recovery process for unclosed journal segments...
2019-03-25 21:20:31,759 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:437 - Successfully started new epoch 1
2019-03-25 21:20:31,760 INFO  namenode.FileJournalManager FileJournalManager.java:recoverUnfinalizedSegments:396 - Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-03-25 21:20:31,760 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1214 - Catching up to latest edits from old active before taking over writer role in edits logs
2019-03-25 21:20:31,764 INFO  blockmanagement.DatanodeManager DatanodeManager.java:markAllDatanodesStale:1837 - Marking all datanodes as stale
2019-03-25 21:20:31,766 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1225 - Reprocessing replication and invalidation queues
2019-03-25 21:20:31,766 INFO  blockmanagement.BlockManager BlockManager.java:initializeReplQueues:4752 - initializing replication queues
2019-03-25 21:20:31,767 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1236 - Will take over writing edit logs at txnid 1
2019-03-25 21:20:31,770 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 1
2019-03-25 21:20:32,229 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:767 - Initializing quota with 4 thread(s)
2019-03-25 21:20:32,234 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:776 - Quota initialization completed in 6 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-03-25 21:20:32,240 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3447 - Total number of blocks            = 0
2019-03-25 21:20:32,240 INFO  blockmanagement.CacheReplicationMonitor CacheReplicationMonitor.java:run:160 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-03-25 21:20:32,240 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3448 - Number of invalid blocks          = 0
2019-03-25 21:20:32,241 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3449 - Number of under-replicated blocks = 0
2019-03-25 21:20:32,241 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3450 - Number of  over-replicated blocks = 0
2019-03-25 21:20:32,241 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3452 - Number of blocks being written    = 0
2019-03-25 21:20:32,241 INFO  hdfs.StateChange BlockManager.java:processMisReplicatesAsync:3455 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 474 msec
2019-03-25 21:20:37,879 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741825_1001, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-25 21:20:37,883 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741826_1002, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-25 21:20:37,884 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741827_1003, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-25 21:20:37,884 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741828_1004, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-25 21:20:37,885 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741829_1005, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-25 21:20:51,038 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741830_1006, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-25 21:20:52,572 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741831_1007, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-25 21:20:52,728 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741832_1008, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-25 21:20:55,053 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741833_1009, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-25 21:21:00,348 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741834_1010, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-25 21:21:03,657 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741835_1011, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-25 21:21:06,560 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741836_1012, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-25 21:21:06,750 ERROR namenode.NameNode LogAdapter.java:error:75 - RECEIVED SIGNAL 15: SIGTERM
2019-03-25 21:21:06,752 INFO  namenode.NameNode LogAdapter.java:info:51 - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode059.clemson.cloudlab.us/130.127.133.68
************************************************************/
2019-03-25 21:21:11,248 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-25 21:21:11,260 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-25 21:21:11,265 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-25 21:21:11,511 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-25 21:21:11,556 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-25 21:21:11,662 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-25 21:21:11,662 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-25 21:21:11,714 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-25 21:21:11,714 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-25 21:21:11,872 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-25 21:21:11,900 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-25 21:21:11,918 INFO  util.log Log.java:initialized:192 - Logging initialized @1186ms
2019-03-25 21:21:12,027 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-25 21:21:12,040 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-25 21:21:12,051 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-25 21:21:12,053 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-25 21:21:12,055 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-25 21:21:12,055 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-25 21:21:12,082 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-25 21:21:12,082 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-25 21:21:12,091 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-25 21:21:12,093 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-25 21:21:12,128 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-25 21:21:12,129 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-25 21:21:12,205 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-25 21:21:12,231 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-25 21:21:12,231 INFO  server.Server Server.java:doStart:414 - Started @1501ms
2019-03-25 21:21:12,579 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-25 21:21:12,629 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-25 21:21:12,644 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-25 21:21:12,645 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-25 21:21:12,648 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-25 21:21:12,655 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-25 21:21:12,655 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-25 21:21:12,655 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-25 21:21:12,655 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-25 21:21:12,655 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-25 21:21:12,698 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-25 21:21:12,711 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-25 21:21:12,711 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-25 21:21:12,715 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-25 21:21:12,716 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 25 21:21:12
2019-03-25 21:21:12,718 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-25 21:21:12,718 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:21:12,720 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-25 21:21:12,720 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-25 21:21:12,850 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-25 21:21:12,859 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-25 21:21:12,860 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-25 21:21:12,860 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-25 21:21:12,860 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-25 21:21:12,860 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-25 21:21:12,860 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-25 21:21:12,860 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-25 21:21:12,860 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-25 21:21:12,861 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-25 21:21:12,861 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = false
2019-03-25 21:21:12,861 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-25 21:21:12,945 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-25 21:21:12,945 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:21:12,945 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-25 21:21:12,945 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-25 21:21:13,019 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-25 21:21:13,019 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-25 21:21:13,019 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-25 21:21:13,020 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-25 21:21:13,027 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-25 21:21:13,029 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-25 21:21:13,035 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-25 21:21:13,035 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:21:13,035 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-25 21:21:13,035 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-25 21:21:13,063 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-25 21:21:13,063 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-25 21:21:13,063 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-25 21:21:13,068 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-25 21:21:13,068 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-25 21:21:13,070 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-25 21:21:13,070 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:21:13,071 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-25 21:21:13,071 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-25 21:21:13,138 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 85305@clnode059.clemson.cloudlab.us
2019-03-25 21:21:14,667 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 1290ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=335ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1073ms
2019-03-25 21:21:17,725 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-25 21:21:17,788 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-25 21:21:17,790 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-25 21:21:17,823 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-25 21:21:17,824 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-25 21:21:17,829 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #1
2019-03-25 21:21:17,829 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-25 21:21:17,833 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 1
2019-03-25 21:21:17,833 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 1
2019-03-25 21:21:17,992 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true of size 1048576 edits # 42 loaded in 0 seconds
2019-03-25 21:21:17,993 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-25 21:21:17,993 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 0 entries 0 lookups
2019-03-25 21:21:17,993 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 4915 msecs
2019-03-25 21:21:18,176 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-25 21:21:18,181 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-25 21:21:18,194 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-25 21:21:18,382 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-25 21:21:18,391 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 5
2019-03-25 21:21:18,402 INFO  hdfs.StateChange BlockManagerSafeMode.java:reportStatus:602 - STATE* Safe mode ON. 
The reported blocks 0 needs additional 6 blocks to reach the threshold 0.9990 of total blocks 7.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-03-25 21:21:18,431 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-25 21:21:18,432 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-25 21:21:18,434 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-25 21:21:18,437 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-25 21:21:18,441 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-25 21:21:18,447 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-25 21:21:19,490 WARN  BlockStateChange BlockManager.java:processIncrementalBlockReport:3959 - BLOCK* processIncrementalBlockReport is received from dead or unregistered node DatanodeRegistration(10.10.1.6:9866, datanodeUuid=2ff9f276-e63a-4764-becf-845c28ea53d9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148)
2019-03-25 21:21:19,490 ERROR BlockStateChange NameNodeRpcServer.java:run:1588 - *BLOCK* NameNode.blockReceivedAndDeleted: failed from DatanodeRegistration(10.10.1.6:9866, datanodeUuid=2ff9f276-e63a-4764-becf-845c28ea53d9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148): Got incremental block report from unregistered or dead node
2019-03-25 21:21:19,543 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e
2019-03-25 21:21:19,544 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.2:9866
2019-03-25 21:21:19,545 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e (10.10.1.2:9866).
2019-03-25 21:21:19,547 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.6:9866, datanodeUuid=2ff9f276-e63a-4764-becf-845c28ea53d9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage 2ff9f276-e63a-4764-becf-845c28ea53d9
2019-03-25 21:21:19,547 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.6:9866
2019-03-25 21:21:19,547 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 2ff9f276-e63a-4764-becf-845c28ea53d9 (10.10.1.6:9866).
2019-03-25 21:21:19,547 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=07af74c7-66ec-4346-ac5d-4cb0b1813108, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage 07af74c7-66ec-4346-ac5d-4cb0b1813108
2019-03-25 21:21:19,548 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.3:9866
2019-03-25 21:21:19,548 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 07af74c7-66ec-4346-ac5d-4cb0b1813108 (10.10.1.3:9866).
2019-03-25 21:21:19,548 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=91a1ec7d-fa15-4ec2-ad49-89860631c88c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage 91a1ec7d-fa15-4ec2-ad49-89860631c88c
2019-03-25 21:21:19,548 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.5:9866
2019-03-25 21:21:19,549 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 91a1ec7d-fa15-4ec2-ad49-89860631c88c (10.10.1.5:9866).
2019-03-25 21:21:19,558 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-0db6f4a6-f885-44e0-baa4-46dcc7f80d5b for DN 10.10.1.6:9866
2019-03-25 21:21:19,561 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-880c1f78-6656-49c2-94e1-a38e8b6d5078 for DN 10.10.1.5:9866
2019-03-25 21:21:19,562 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-44cb71b1-9161-4abf-b5a2-a65184867edd for DN 10.10.1.3:9866
2019-03-25 21:21:19,562 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-ceafbb3d-e9f8-4658-a7a8-e99b7369ec97 for DN 10.10.1.2:9866
2019-03-25 21:21:19,575 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xfd7e4e9b601ea2c3: Processing first storage report for DS-44cb71b1-9161-4abf-b5a2-a65184867edd from datanode 07af74c7-66ec-4346-ac5d-4cb0b1813108
2019-03-25 21:21:19,581 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xfd7e4e9b601ea2c3: from storage DS-44cb71b1-9161-4abf-b5a2-a65184867edd node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=07af74c7-66ec-4346-ac5d-4cb0b1813108, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 11, hasStaleStorage: false, processing time: 7 msecs, invalidatedBlocks: 0
2019-03-25 21:21:19,582 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xbfe0a867b0ef729d: Processing first storage report for DS-0db6f4a6-f885-44e0-baa4-46dcc7f80d5b from datanode 2ff9f276-e63a-4764-becf-845c28ea53d9
2019-03-25 21:21:19,582 INFO  hdfs.StateChange BlockManagerSafeMode.java:reportStatus:602 - STATE* Safe mode extension entered. 
The reported blocks 6 has reached the threshold 0.9990 of total blocks 7. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-03-25 21:21:19,583 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xbfe0a867b0ef729d: from storage DS-0db6f4a6-f885-44e0-baa4-46dcc7f80d5b node DatanodeRegistration(10.10.1.6:9866, datanodeUuid=2ff9f276-e63a-4764-becf-845c28ea53d9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 6, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2019-03-25 21:21:19,583 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xfc91d2945a263c0a: Processing first storage report for DS-880c1f78-6656-49c2-94e1-a38e8b6d5078 from datanode 91a1ec7d-fa15-4ec2-ad49-89860631c88c
2019-03-25 21:21:19,584 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xfc91d2945a263c0a: from storage DS-880c1f78-6656-49c2-94e1-a38e8b6d5078 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=91a1ec7d-fa15-4ec2-ad49-89860631c88c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 13, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2019-03-25 21:21:23,255 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xcce998643b868ac3: Processing first storage report for DS-ceafbb3d-e9f8-4658-a7a8-e99b7369ec97 from datanode dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e
2019-03-25 21:21:23,255 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xcce998643b868ac3: from storage DS-ceafbb3d-e9f8-4658-a7a8-e99b7369ec97 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 11, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2019-03-25 21:21:36,174 INFO  namenode.FSNamesystem FSNamesystem.java:stopStandbyServices:1408 - Stopping services started for standby state
2019-03-25 21:21:36,175 WARN  ha.EditLogTailer EditLogTailer.java:doWork:481 - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-03-25 21:21:36,179 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1203 - Starting services required for active state
2019-03-25 21:21:36,187 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:435 - Starting recovery process for unclosed journal segments...
2019-03-25 21:21:43,366 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:437 - Successfully started new epoch 3
2019-03-25 21:21:43,366 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:277 - Beginning recovery of unclosed segment starting at txid 43
2019-03-25 21:21:43,387 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:286 - Recovery prepare phase complete. Responses:
10.10.1.5:8485: segmentState { startTxId: 43 endTxId: 58 isInProgress: true } lastWriterEpoch: 2 lastCommittedTxId: 52
10.10.1.2:8485: segmentState { startTxId: 43 endTxId: 58 isInProgress: true } lastWriterEpoch: 2 lastCommittedTxId: 52
2019-03-25 21:21:43,389 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:310 - Using longest log: 10.10.1.5:8485=segmentState {
  startTxId: 43
  endTxId: 58
  isInProgress: true
}
lastWriterEpoch: 2
lastCommittedTxId: 52

2019-03-25 21:21:45,453 INFO  namenode.FileJournalManager FileJournalManager.java:recoverUnfinalizedSegments:396 - Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-03-25 21:21:45,468 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000001-0000000000000000042
2019-03-25 21:21:45,484 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1214 - Catching up to latest edits from old active before taking over writer role in edits logs
2019-03-25 21:21:45,489 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@580d489f expecting start txid #43
2019-03-25 21:21:45,489 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-25 21:21:45,489 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 43
2019-03-25 21:21:45,490 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 43
2019-03-25 21:21:45,500 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true of size 1048576 edits # 16 loaded in 0 seconds
2019-03-25 21:21:45,500 INFO  blockmanagement.DatanodeManager DatanodeManager.java:markAllDatanodesStale:1837 - Marking all datanodes as stale
2019-03-25 21:21:45,501 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1236 - Will take over writing edit logs at txnid 59
2019-03-25 21:21:45,503 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 59
2019-03-25 21:21:45,775 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:767 - Initializing quota with 4 thread(s)
2019-03-25 21:21:45,779 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:776 - Quota initialization completed in 4 milliseconds
name space=6
storage space=6845104128
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-03-25 21:21:45,785 INFO  hdfs.StateChange BlockManagerSafeMode.java:reportStatus:602 - STATE* Safe mode ON. 
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds.
2019-03-25 21:21:45,785 INFO  blockmanagement.CacheReplicationMonitor CacheReplicationMonitor.java:run:160 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-03-25 21:21:45,785 INFO  namenode.FSNamesystem FSNamesystemLock.java:writeUnlock:282 - FSNamesystem write lock held for 9610 ms via
java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1032)
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock(FSNamesystemLock.java:284)
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock(FSNamesystemLock.java:224)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock(FSNamesystem.java:1603)
org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.writeUnlock(NameNode.java:1945)
org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:67)
org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1746)
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1723)
org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
java.security.AccessController.doPrivileged(Native Method)
javax.security.auth.Subject.doAs(Subject.java:422)
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
	Number of suppressed write-lock reports: 0
	Longest write-lock held interval: 9610
2019-03-25 21:21:45,786 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 6 on 8020, call Call#10 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41284
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:45,786 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 0 on 8020, call Call#11 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41282
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:45,786 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 4 on 8020, call Call#12 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41282
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-1849486229_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-1849486229_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:45,786 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 9 on 8020, call Call#10 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41288
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:45,802 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 1 on 8020, call Call#10 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41292
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_1293668592_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_1293668592_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:45,802 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 7 on 8020, call Call#11 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41296
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_97505575_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_97505575_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:45,802 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 5 on 8020, call Call#11 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41284
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-441379747_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-441379747_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:45,802 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 3 on 8020, call Call#12 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41296
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:45,802 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 2 on 8020, call Call#11 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41288
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_932206338_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_932206338_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:45,803 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 9 on 8020, call Call#12 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41282
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-1849486229_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-1849486229_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:45,807 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 1 on 8020, call Call#11 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41284
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-441379747_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-441379747_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:45,808 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 6 on 8020, call Call#11 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41288
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_932206338_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_932206338_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:45,808 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 7 on 8020, call Call#12 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41296
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:46,319 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 8 on 8020, call Call#12 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41282
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-1849486229_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-1849486229_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:46,385 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 4 on 8020, call Call#11 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41282
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:46,479 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 0 on 8020, call Call#12 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41296
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:46,513 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 5 on 8020, call Call#11 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41288
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_932206338_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_932206338_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:46,553 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 3 on 8020, call Call#11 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41296
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_97505575_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_97505575_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:46,679 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 2 on 8020, call Call#10 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41292
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_1293668592_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_1293668592_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:46,730 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 9 on 8020, call Call#10 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41288
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:46,974 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 8 on 8020, call Call#11 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41284
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-441379747_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-441379747_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:47,151 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 4 on 8020, call Call#10 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41284
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:47,444 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 0 on 8020, call Call#12 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41282
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-1849486229_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-1849486229_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:47,570 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 2 on 8020, call Call#11 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41288
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_932206338_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_932206338_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:47,979 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 4 on 8020, call Call#11 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41282
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 1 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 1 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:48,151 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 0 on 8020, call Call#11 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41284
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-441379747_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 1 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_-441379747_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 1 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:48,507 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 5 on 8020, call Call#10 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41284
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 1 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 1 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:48,838 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 5 on 8020, call Call#12 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41296
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:49,091 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 3 on 8020, call Call#11 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41296
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_97505575_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_97505575_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:49,425 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 2 on 8020, call Call#10 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41288
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:21:49,521 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 9 on 8020, call Call#10 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from 10.10.1.7:41292
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_1293668592_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:3744)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:1134)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewLease(ClientNamenodeProtocolServerSideTranslatorPB.java:737)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_NONMAPREDUCE_1293668592_1. Name node is in safe mode.
The reported blocks 12 has reached the threshold 0.9990 of total blocks 12. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 12 more
2019-03-25 21:21:49,786 INFO  blockmanagement.BlockManager BlockManager.java:initializeReplQueues:4752 - initializing replication queues
2019-03-25 21:21:49,786 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:391 - STATE* Safe mode is OFF
2019-03-25 21:21:49,787 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 31 secs
2019-03-25 21:21:49,787 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 1 racks and 4 datanodes
2019-03-25 21:21:49,787 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-25 21:21:49,848 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3447 - Total number of blocks            = 17
2019-03-25 21:21:49,848 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3448 - Number of invalid blocks          = 0
2019-03-25 21:21:49,848 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3449 - Number of under-replicated blocks = 0
2019-03-25 21:21:49,848 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3450 - Number of  over-replicated blocks = 0
2019-03-25 21:21:49,848 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3452 - Number of blocks being written    = 5
2019-03-25 21:21:49,848 INFO  hdfs.StateChange BlockManager.java:processMisReplicatesAsync:3455 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 62 msec
2019-03-25 21:21:51,676 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741842_1018, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-25 21:21:52,284 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741843_1019, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-25 21:21:52,787 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741844_1020, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-25 21:21:52,841 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741845_1021, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-25 21:21:54,181 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741846_1022, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-25 21:22:01,097 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 6915ms to send a batch of 2 edits (50 bytes) to remote journal 10.10.1.2:8485
2019-03-25 21:22:03,413 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 2315ms to send a batch of 1 edits (74 bytes) to remote journal 10.10.1.2:8485
2019-03-25 21:22:05,781 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 5804ms to send a batch of 1 edits (74 bytes) to remote journal 10.10.1.3:8485
2019-03-25 21:22:08,507 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741847_1023, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-25 21:22:08,509 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741848_1024, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-25 21:22:08,509 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741849_1025, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-25 21:22:11,096 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 2588ms to send a batch of 2 edits (50 bytes) to remote journal 10.10.1.3:8485
2019-03-25 21:22:13,096 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 1991ms to send a batch of 7 edits (322 bytes) to remote journal 10.10.1.3:8485
2019-03-25 21:22:13,509 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 5001ms to send a batch of 2 edits (50 bytes) to remote journal 10.10.1.2:8485
2019-03-25 21:22:15,927 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741850_1026, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-25 21:22:15,927 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 27 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 70 Number of syncs: 13 SyncTimes(ms): 13924 143 
2019-03-25 21:22:21,087 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741851_1027, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-25 21:22:27,553 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741852_1028, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-25 21:22:29,390 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741853_1029, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-25 21:22:29,765 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 2212ms to send a batch of 2 edits (50 bytes) to remote journal 10.10.1.2:8485
2019-03-25 21:22:30,047 ERROR namenode.NameNode LogAdapter.java:error:75 - RECEIVED SIGNAL 15: SIGTERM
2019-03-25 21:22:30,049 INFO  namenode.NameNode LogAdapter.java:info:51 - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode059.clemson.cloudlab.us/130.127.133.68
************************************************************/
2019-03-25 21:22:34,155 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-25 21:22:34,167 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-25 21:22:34,172 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-25 21:22:34,416 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-25 21:22:34,460 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-25 21:22:34,564 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-25 21:22:34,564 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-25 21:22:34,616 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-25 21:22:34,616 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-25 21:22:34,779 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-25 21:22:34,806 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-25 21:22:34,824 INFO  util.log Log.java:initialized:192 - Logging initialized @1183ms
2019-03-25 21:22:34,932 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-25 21:22:34,945 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-25 21:22:34,956 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-25 21:22:34,959 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-25 21:22:34,961 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-25 21:22:34,961 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-25 21:22:34,988 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-25 21:22:34,988 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-25 21:22:34,997 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-25 21:22:34,998 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-25 21:22:35,033 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-25 21:22:35,034 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-25 21:22:35,109 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-25 21:22:35,127 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-25 21:22:35,127 INFO  server.Server Server.java:doStart:414 - Started @1488ms
2019-03-25 21:22:35,456 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-25 21:22:35,514 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-25 21:22:35,529 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-25 21:22:35,530 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-25 21:22:35,533 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-25 21:22:35,540 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-25 21:22:35,540 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-25 21:22:35,540 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-25 21:22:35,541 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-25 21:22:35,541 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-25 21:22:35,584 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-25 21:22:35,596 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-25 21:22:35,596 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-25 21:22:35,601 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-25 21:22:35,602 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 25 21:22:35
2019-03-25 21:22:35,604 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-25 21:22:35,604 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:22:35,606 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-25 21:22:35,606 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-25 21:22:35,742 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-25 21:22:35,751 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-25 21:22:35,752 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-25 21:22:35,752 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-25 21:22:35,752 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-25 21:22:35,752 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-25 21:22:35,752 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-25 21:22:35,752 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-25 21:22:35,753 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-25 21:22:35,753 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-25 21:22:35,753 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = false
2019-03-25 21:22:35,753 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-25 21:22:35,837 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-25 21:22:35,837 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:22:35,837 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-25 21:22:35,837 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-25 21:22:35,911 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-25 21:22:35,911 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-25 21:22:35,911 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-25 21:22:35,911 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-25 21:22:35,919 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-25 21:22:35,921 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-25 21:22:35,927 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-25 21:22:35,927 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:22:35,927 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-25 21:22:35,927 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-25 21:22:35,955 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-25 21:22:35,955 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-25 21:22:35,955 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-25 21:22:35,960 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-25 21:22:35,960 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-25 21:22:35,962 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-25 21:22:35,962 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-25 21:22:35,963 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-25 21:22:35,963 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-25 21:22:36,047 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 86992@clnode059.clemson.cloudlab.us
2019-03-25 21:22:37,607 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 1324ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=337ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1087ms
2019-03-25 21:22:37,776 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-25 21:22:37,841 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-25 21:22:37,843 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-25 21:22:37,874 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-25 21:22:37,875 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-25 21:22:37,880 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #1
2019-03-25 21:22:37,880 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-25 21:22:37,884 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 1
2019-03-25 21:22:37,884 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 1
2019-03-25 21:22:37,964 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true of size 1048576 edits # 42 loaded in 0 seconds
2019-03-25 21:22:37,965 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4044fb95 expecting start txid #43
2019-03-25 21:22:37,965 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-25 21:22:37,965 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 1
2019-03-25 21:22:37,965 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 1
2019-03-25 21:22:37,970 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=43&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true of size 1048576 edits # 16 loaded in 0 seconds
2019-03-25 21:22:37,970 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #59
2019-03-25 21:22:37,970 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-25 21:22:37,970 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 1
2019-03-25 21:22:37,971 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 1
2019-03-25 21:22:37,977 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=59&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true of size 1048576 edits # 33 loaded in 0 seconds
2019-03-25 21:22:37,977 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-25 21:22:37,977 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 0 entries 0 lookups
2019-03-25 21:22:37,977 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 2007 msecs
2019-03-25 21:22:38,168 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-25 21:22:38,174 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-25 21:22:38,186 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-25 21:22:38,375 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-25 21:22:38,384 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 5
2019-03-25 21:22:38,396 INFO  hdfs.StateChange BlockManagerSafeMode.java:reportStatus:602 - STATE* Safe mode ON. 
The reported blocks 0 needs additional 21 blocks to reach the threshold 0.9990 of total blocks 22.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2019-03-25 21:22:38,426 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-25 21:22:38,426 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-25 21:22:38,428 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-25 21:22:38,431 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-25 21:22:38,436 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-25 21:22:38,442 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-25 21:22:38,856 WARN  BlockStateChange BlockManager.java:processIncrementalBlockReport:3959 - BLOCK* processIncrementalBlockReport is received from dead or unregistered node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=07af74c7-66ec-4346-ac5d-4cb0b1813108, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148)
2019-03-25 21:22:38,856 ERROR BlockStateChange NameNodeRpcServer.java:run:1588 - *BLOCK* NameNode.blockReceivedAndDeleted: failed from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=07af74c7-66ec-4346-ac5d-4cb0b1813108, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148): Got incremental block report from unregistered or dead node
2019-03-25 21:22:38,857 WARN  BlockStateChange BlockManager.java:processIncrementalBlockReport:3959 - BLOCK* processIncrementalBlockReport is received from dead or unregistered node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148)
2019-03-25 21:22:38,857 ERROR BlockStateChange NameNodeRpcServer.java:run:1588 - *BLOCK* NameNode.blockReceivedAndDeleted: failed from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148): Got incremental block report from unregistered or dead node
2019-03-25 21:22:38,903 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=91a1ec7d-fa15-4ec2-ad49-89860631c88c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage 91a1ec7d-fa15-4ec2-ad49-89860631c88c
2019-03-25 21:22:38,905 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.5:9866
2019-03-25 21:22:38,905 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 91a1ec7d-fa15-4ec2-ad49-89860631c88c (10.10.1.5:9866).
2019-03-25 21:22:38,907 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e
2019-03-25 21:22:38,907 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.2:9866
2019-03-25 21:22:38,907 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e (10.10.1.2:9866).
2019-03-25 21:22:38,908 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=07af74c7-66ec-4346-ac5d-4cb0b1813108, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage 07af74c7-66ec-4346-ac5d-4cb0b1813108
2019-03-25 21:22:38,908 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.3:9866
2019-03-25 21:22:38,908 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 07af74c7-66ec-4346-ac5d-4cb0b1813108 (10.10.1.3:9866).
2019-03-25 21:22:38,909 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.6:9866, datanodeUuid=2ff9f276-e63a-4764-becf-845c28ea53d9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148) storage 2ff9f276-e63a-4764-becf-845c28ea53d9
2019-03-25 21:22:38,909 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.6:9866
2019-03-25 21:22:38,909 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 2ff9f276-e63a-4764-becf-845c28ea53d9 (10.10.1.6:9866).
2019-03-25 21:22:38,919 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-880c1f78-6656-49c2-94e1-a38e8b6d5078 for DN 10.10.1.5:9866
2019-03-25 21:22:38,922 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-ceafbb3d-e9f8-4658-a7a8-e99b7369ec97 for DN 10.10.1.2:9866
2019-03-25 21:22:38,923 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-44cb71b1-9161-4abf-b5a2-a65184867edd for DN 10.10.1.3:9866
2019-03-25 21:22:38,924 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-0db6f4a6-f885-44e0-baa4-46dcc7f80d5b for DN 10.10.1.6:9866
2019-03-25 21:22:38,936 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xfd7e4e9b601ea2c4: Processing first storage report for DS-44cb71b1-9161-4abf-b5a2-a65184867edd from datanode 07af74c7-66ec-4346-ac5d-4cb0b1813108
2019-03-25 21:22:38,943 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xfd7e4e9b601ea2c4: from storage DS-44cb71b1-9161-4abf-b5a2-a65184867edd node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=07af74c7-66ec-4346-ac5d-4cb0b1813108, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 24, hasStaleStorage: false, processing time: 7 msecs, invalidatedBlocks: 0
2019-03-25 21:22:38,943 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xbfe0a867b0ef729e: Processing first storage report for DS-0db6f4a6-f885-44e0-baa4-46dcc7f80d5b from datanode 2ff9f276-e63a-4764-becf-845c28ea53d9
2019-03-25 21:22:38,944 INFO  hdfs.StateChange BlockManagerSafeMode.java:reportStatus:602 - STATE* Safe mode extension entered. 
The reported blocks 21 has reached the threshold 0.9990 of total blocks 22. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2019-03-25 21:22:38,945 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xbfe0a867b0ef729e: from storage DS-0db6f4a6-f885-44e0-baa4-46dcc7f80d5b node DatanodeRegistration(10.10.1.6:9866, datanodeUuid=2ff9f276-e63a-4764-becf-845c28ea53d9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 22, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-03-25 21:22:38,945 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xfc91d2945a263c0b: Processing first storage report for DS-880c1f78-6656-49c2-94e1-a38e8b6d5078 from datanode 91a1ec7d-fa15-4ec2-ad49-89860631c88c
2019-03-25 21:22:38,946 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xfc91d2945a263c0b: from storage DS-880c1f78-6656-49c2-94e1-a38e8b6d5078 node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=91a1ec7d-fa15-4ec2-ad49-89860631c88c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 27, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2019-03-25 21:22:38,947 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xcce998643b868ac4: Processing first storage report for DS-ceafbb3d-e9f8-4658-a7a8-e99b7369ec97 from datanode dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e
2019-03-25 21:22:38,948 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xcce998643b868ac4: from storage DS-ceafbb3d-e9f8-4658-a7a8-e99b7369ec97 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=dfa2f45b-eaa7-42f5-883b-18a1fbc8f71e, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-33de1c13-dc26-4659-9ce0-787b2506677e;nsid=1221111059;c=1553570393148), blocks: 23, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2019-03-25 21:22:57,254 INFO  namenode.FSNamesystem FSNamesystem.java:stopStandbyServices:1408 - Stopping services started for standby state
2019-03-25 21:22:57,256 WARN  ha.EditLogTailer EditLogTailer.java:doWork:481 - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-03-25 21:22:57,258 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1203 - Starting services required for active state
2019-03-25 21:22:57,268 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:435 - Starting recovery process for unclosed journal segments...
2019-03-25 21:23:00,340 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:437 - Successfully started new epoch 5
2019-03-25 21:23:00,341 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:277 - Beginning recovery of unclosed segment starting at txid 92
2019-03-25 21:23:00,361 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:286 - Recovery prepare phase complete. Responses:
10.10.1.5:8485: segmentState { startTxId: 92 endTxId: 116 isInProgress: true } lastWriterEpoch: 4 lastCommittedTxId: 109
10.10.1.2:8485: segmentState { startTxId: 92 endTxId: 116 isInProgress: true } lastWriterEpoch: 4 lastCommittedTxId: 109
2019-03-25 21:23:00,363 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:310 - Using longest log: 10.10.1.5:8485=segmentState {
  startTxId: 92
  endTxId: 116
  isInProgress: true
}
lastWriterEpoch: 4
lastCommittedTxId: 109

2019-03-25 21:23:00,426 INFO  namenode.FileJournalManager FileJournalManager.java:recoverUnfinalizedSegments:396 - Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-03-25 21:23:00,440 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000059 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000059-0000000000000000089
2019-03-25 21:23:00,457 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1214 - Catching up to latest edits from old active before taking over writer role in edits logs
2019-03-25 21:23:00,461 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2c3ab350 expecting start txid #92
2019-03-25 21:23:00,461 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=92&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=92&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-25 21:23:00,462 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=92&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=92&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 92
2019-03-25 21:23:00,462 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=92&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true' to transaction ID 92
2019-03-25 21:23:00,552 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=92&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=92&storageInfo=-64%3A1221111059%3A1553570393148%3ACID-33de1c13-dc26-4659-9ce0-787b2506677e&inProgressOk=true of size 1048576 edits # 25 loaded in 0 seconds
2019-03-25 21:23:00,553 INFO  blockmanagement.DatanodeManager DatanodeManager.java:markAllDatanodesStale:1837 - Marking all datanodes as stale
2019-03-25 21:23:00,554 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1236 - Will take over writing edit logs at txnid 117
2019-03-25 21:23:00,558 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 117
2019-03-25 21:23:00,852 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:767 - Initializing quota with 4 thread(s)
2019-03-25 21:23:00,856 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:776 - Quota initialization completed in 4 milliseconds
name space=6
storage space=14092861440
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-03-25 21:23:00,862 INFO  blockmanagement.CacheReplicationMonitor CacheReplicationMonitor.java:run:160 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-03-25 21:23:00,862 INFO  hdfs.StateChange BlockManagerSafeMode.java:reportStatus:602 - STATE* Safe mode ON. 
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 8 seconds.
2019-03-25 21:23:00,863 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 6 on 8020, call Call#16 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41382
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 8 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 8 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:00,863 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 3 on 8020, call Call#16 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41378
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 8 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 8 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:00,863 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 1 on 8020, call Call#15 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41376
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 8 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 8 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:00,863 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 9 on 8020, call Call#16 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41380
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 8 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 8 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:01,087 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 5 on 8020, call Call#16 Retry#1 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41392
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile3._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 7 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile3._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 7 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:02,473 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 7 on 8020, call Call#16 Retry#2 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41392
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile3._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 6 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile3._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 6 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:04,639 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 6 on 8020, call Call#16 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41378
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:04,676 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 8 on 8020, call Call#16 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41380
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile4._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:04,812 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 3 on 8020, call Call#16 Retry#3 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41392
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile3._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile3._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:05,480 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 6 on 8020, call Call#16 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41382
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile5._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 3 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:06,191 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 2 on 8020, call Call#15 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41376
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile2._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 2 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:08,926 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 8 on 8020, call Call#16 Retry#5 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41378
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile1._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:09,253 INFO  ipc.Server Server.java:logException:2722 - IPC Server handler 3 on 8020, call Call#16 Retry#4 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.10.1.7:41392
org.apache.hadoop.ipc.RetriableException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile3._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1441)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:591)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot add block to /myfile3._COPYING_. Name node is in safe mode.
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 4 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds. NamenodeHostName:node-0-link-0
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1450)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1437)
	... 14 more
2019-03-25 21:23:09,864 INFO  blockmanagement.BlockManager BlockManager.java:initializeReplQueues:4752 - initializing replication queues
2019-03-25 21:23:09,864 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:391 - STATE* Safe mode is OFF
2019-03-25 21:23:09,864 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 31 secs
2019-03-25 21:23:09,865 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 1 racks and 4 datanodes
2019-03-25 21:23:09,865 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-25 21:23:09,931 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3447 - Total number of blocks            = 35
2019-03-25 21:23:09,931 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3448 - Number of invalid blocks          = 0
2019-03-25 21:23:09,932 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3449 - Number of under-replicated blocks = 0
2019-03-25 21:23:09,932 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3450 - Number of  over-replicated blocks = 0
2019-03-25 21:23:09,932 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3452 - Number of blocks being written    = 5
2019-03-25 21:23:09,932 INFO  hdfs.StateChange BlockManager.java:processMisReplicatesAsync:3455 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 67 msec
2019-03-25 21:23:13,300 INFO  hdfs.StateChange FSDirWriteFileOp.java:analyzeFileState:648 - BLOCK* allocateBlock: caught retry for allocation of a new block in /myfile5._COPYING_. Returning previously allocated block blk_1073741860_1036
2019-03-25 21:23:13,308 INFO  hdfs.StateChange FSDirWriteFileOp.java:analyzeFileState:648 - BLOCK* allocateBlock: caught retry for allocation of a new block in /myfile5._COPYING_. Returning previously allocated block blk_1073741860_1036
2019-03-25 21:23:13,400 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741861_1037, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-25 21:23:14,947 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741862_1038, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-25 21:23:16,265 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 2864ms to send a batch of 2 edits (50 bytes) to remote journal 10.10.1.5:8485
2019-03-25 21:23:18,135 INFO  hdfs.StateChange FSDirWriteFileOp.java:analyzeFileState:648 - BLOCK* allocateBlock: caught retry for allocation of a new block in /myfile2._COPYING_. Returning previously allocated block blk_1073741858_1034
2019-03-25 21:23:18,136 INFO  hdfs.StateChange FSDirWriteFileOp.java:analyzeFileState:648 - BLOCK* allocateBlock: caught retry for allocation of a new block in /myfile2._COPYING_. Returning previously allocated block blk_1073741858_1034
2019-03-25 21:23:18,997 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 5596ms to send a batch of 2 edits (50 bytes) to remote journal 10.10.1.3:8485
2019-03-25 21:23:20,144 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 6742ms to send a batch of 2 edits (50 bytes) to remote journal 10.10.1.2:8485
2019-03-25 21:23:23,053 WARN  client.QuorumJournalManager IPCLoggerChannel.java:call:417 - Took 4050ms to send a batch of 4 edits (198 bytes) to remote journal 10.10.1.5:8485
2019-03-25 21:23:24,617 INFO  hdfs.StateChange FSDirWriteFileOp.java:analyzeFileState:648 - BLOCK* allocateBlock: caught retry for allocation of a new block in /myfile1._COPYING_. Returning previously allocated block blk_1073741859_1035
2019-03-25 21:23:24,618 INFO  hdfs.StateChange FSDirWriteFileOp.java:analyzeFileState:648 - BLOCK* allocateBlock: caught retry for allocation of a new block in /myfile1._COPYING_. Returning previously allocated block blk_1073741859_1035
2019-03-25 21:23:29,776 INFO  hdfs.StateChange FSNamesystem.java:completeFile:2850 - DIR* completeFile: /myfile5._COPYING_ is closed by DFSClient_NONMAPREDUCE_97505575_1
2019-03-25 21:23:30,368 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741863_1039, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-25 21:23:35,327 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741864_1040, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-25 21:23:37,028 INFO  hdfs.StateChange FSNamesystem.java:completeFile:2850 - DIR* completeFile: /myfile1._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1849486229_1
2019-03-25 21:23:37,028 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 16 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 122 Number of syncs: 9 SyncTimes(ms): 6847 121 
2019-03-25 21:23:42,940 INFO  hdfs.StateChange FSNamesystem.java:completeFile:2850 - DIR* completeFile: /myfile2._COPYING_ is closed by DFSClient_NONMAPREDUCE_-441379747_1
2019-03-25 21:23:43,423 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741865_1041, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile3._COPYING_
