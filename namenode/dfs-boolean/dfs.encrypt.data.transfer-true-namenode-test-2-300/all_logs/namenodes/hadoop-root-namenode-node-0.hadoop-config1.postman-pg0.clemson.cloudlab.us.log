2019-03-20 15:51:46,541 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-20 15:51:46,553 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-20 15:51:46,558 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-20 15:51:46,802 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-20 15:51:46,846 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-20 15:51:46,952 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-20 15:51:46,952 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-20 15:51:47,007 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-20 15:51:47,007 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-20 15:51:47,170 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-20 15:51:47,198 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-20 15:51:47,215 INFO  util.log Log.java:initialized:192 - Logging initialized @1185ms
2019-03-20 15:51:47,324 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-20 15:51:47,337 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-20 15:51:47,348 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-20 15:51:47,350 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-20 15:51:47,352 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-20 15:51:47,352 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-20 15:51:47,378 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-20 15:51:47,379 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-20 15:51:47,388 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-20 15:51:47,389 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-20 15:51:47,425 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-20 15:51:47,425 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-20 15:51:47,500 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-20 15:51:47,523 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-20 15:51:47,523 INFO  server.Server Server.java:doStart:414 - Started @1495ms
2019-03-20 15:51:47,853 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-20 15:51:47,904 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-20 15:51:47,919 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-20 15:51:47,920 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-20 15:51:47,923 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-20 15:51:47,930 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-20 15:51:47,930 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-20 15:51:47,930 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-20 15:51:47,930 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-20 15:51:47,930 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-20 15:51:47,973 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-20 15:51:47,986 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-20 15:51:47,986 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-20 15:51:47,991 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-20 15:51:47,991 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 20 15:51:47
2019-03-20 15:51:47,993 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-20 15:51:47,994 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:51:47,995 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-20 15:51:47,995 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-20 15:51:48,130 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-20 15:51:48,139 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-20 15:51:48,140 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-20 15:51:48,140 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-20 15:51:48,140 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-20 15:51:48,140 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-20 15:51:48,140 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-20 15:51:48,140 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-20 15:51:48,141 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-20 15:51:48,141 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-20 15:51:48,141 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = true
2019-03-20 15:51:48,141 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-20 15:51:48,225 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-20 15:51:48,226 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:51:48,226 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-20 15:51:48,226 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-20 15:51:48,301 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-20 15:51:48,301 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-20 15:51:48,301 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-20 15:51:48,302 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-20 15:51:48,309 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-20 15:51:48,311 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-20 15:51:48,317 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-20 15:51:48,317 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:51:48,317 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-20 15:51:48,317 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-20 15:51:48,345 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-20 15:51:48,346 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-20 15:51:48,346 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-20 15:51:48,350 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-20 15:51:48,350 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-20 15:51:48,353 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-20 15:51:48,353 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:51:48,353 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-20 15:51:48,353 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-20 15:51:48,414 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 117416@clnode059.clemson.cloudlab.us
2019-03-20 15:51:49,875 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 1201ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=342ms
GC pool 'PS Scavenge' had collection(s): count=1 time=996ms
2019-03-20 15:51:50,028 INFO  namenode.FSImage FSImage.java:loadFSImage:718 - No edit log streams selected.
2019-03-20 15:51:50,029 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-20 15:51:50,091 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-20 15:51:50,094 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-20 15:51:50,124 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-20 15:51:50,125 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-20 15:51:50,130 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-20 15:51:50,130 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 0 entries 0 lookups
2019-03-20 15:51:50,130 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 1768 msecs
2019-03-20 15:51:50,309 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-20 15:51:50,313 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-20 15:51:50,326 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-20 15:51:50,513 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-20 15:51:50,521 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 0
2019-03-20 15:51:50,531 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 0 secs
2019-03-20 15:51:50,532 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 0 racks and 0 datanodes
2019-03-20 15:51:50,532 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-20 15:51:50,566 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-20 15:51:50,566 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-20 15:51:50,569 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-20 15:51:50,572 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-20 15:51:50,576 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-20 15:51:50,583 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-20 15:51:55,495 INFO  namenode.TransferFsImage TransferFsImage.java:copyFileToStream:396 - Sending fileName: /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, fileSize: 389. Sent total: 389 bytes. Size of last segment intended to send: -1 bytes.
2019-03-20 15:51:55,670 ERROR namenode.NameNode LogAdapter.java:error:75 - RECEIVED SIGNAL 15: SIGTERM
2019-03-20 15:51:55,672 INFO  namenode.NameNode LogAdapter.java:info:51 - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode059.clemson.cloudlab.us/130.127.133.68
************************************************************/
2019-03-20 15:52:11,592 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-20 15:52:11,604 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-20 15:52:11,609 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-20 15:52:11,853 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-20 15:52:11,897 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-20 15:52:12,002 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-20 15:52:12,002 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-20 15:52:12,056 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-20 15:52:12,056 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-20 15:52:12,215 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-20 15:52:12,243 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-20 15:52:12,261 INFO  util.log Log.java:initialized:192 - Logging initialized @1187ms
2019-03-20 15:52:12,369 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-20 15:52:12,382 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-20 15:52:12,393 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-20 15:52:12,396 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-20 15:52:12,398 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-20 15:52:12,398 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-20 15:52:12,424 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-20 15:52:12,425 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-20 15:52:12,434 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-20 15:52:12,435 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-20 15:52:12,471 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-20 15:52:12,471 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-20 15:52:12,546 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-20 15:52:12,567 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-20 15:52:12,567 INFO  server.Server Server.java:doStart:414 - Started @1495ms
2019-03-20 15:52:12,898 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-20 15:52:12,955 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-20 15:52:12,969 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-20 15:52:12,971 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-20 15:52:12,974 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-20 15:52:12,980 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-20 15:52:12,980 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-20 15:52:12,980 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-20 15:52:12,981 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-20 15:52:12,981 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-20 15:52:13,024 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-20 15:52:13,036 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-20 15:52:13,036 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-20 15:52:13,041 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-20 15:52:13,042 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 20 15:52:13
2019-03-20 15:52:13,044 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-20 15:52:13,044 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:52:13,046 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-20 15:52:13,046 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-20 15:52:13,184 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-20 15:52:13,193 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-20 15:52:13,194 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-20 15:52:13,194 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-20 15:52:13,194 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-20 15:52:13,194 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-20 15:52:13,194 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-20 15:52:13,194 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-20 15:52:13,195 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-20 15:52:13,195 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-20 15:52:13,195 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = true
2019-03-20 15:52:13,195 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-20 15:52:13,279 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-20 15:52:13,279 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:52:13,279 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-20 15:52:13,279 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-20 15:52:13,352 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-20 15:52:13,353 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-20 15:52:13,353 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-20 15:52:13,353 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-20 15:52:13,360 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-20 15:52:13,363 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-20 15:52:13,368 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-20 15:52:13,369 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:52:13,369 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-20 15:52:13,369 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-20 15:52:13,397 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-20 15:52:13,397 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-20 15:52:13,397 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-20 15:52:13,401 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-20 15:52:13,402 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-20 15:52:13,404 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-20 15:52:13,404 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:52:13,404 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-20 15:52:13,404 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-20 15:52:13,475 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 118099@clnode059.clemson.cloudlab.us
2019-03-20 15:52:15,023 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 1303ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=329ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1097ms
2019-03-20 15:52:16,142 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:16,142 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:16,142 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:17,143 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:17,143 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:17,143 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:18,144 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:18,144 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:18,144 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:19,145 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:19,145 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:19,145 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:19,579 INFO  client.QuorumJournalManager QuorumCall.java:waitFor:187 - Waited 6002 ms (timeout=20000 ms) for a response for selectInputStreams. No responses yet.
2019-03-20 15:52:20,146 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-2-link-0/10.10.1.5:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:20,146 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-4-link-0/10.10.1.2:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:20,146 INFO  ipc.Client Client.java:handleConnectionFailure:942 - Retrying connect to server: node-3-link-0/10.10.1.3:8485. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-03-20 15:52:20,581 INFO  client.QuorumJournalManager QuorumCall.java:waitFor:187 - Waited 7003 ms (timeout=20000 ms) for a response for selectInputStreams. No responses yet.
2019-03-20 15:52:20,712 INFO  namenode.FSImage FSImage.java:loadFSImage:718 - No edit log streams selected.
2019-03-20 15:52:20,713 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-20 15:52:20,775 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-20 15:52:20,777 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-20 15:52:20,810 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-20 15:52:20,810 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-20 15:52:20,815 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-20 15:52:20,816 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 0 entries 0 lookups
2019-03-20 15:52:20,816 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 7403 msecs
2019-03-20 15:52:20,999 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-20 15:52:21,004 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-20 15:52:21,017 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-20 15:52:21,202 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-20 15:52:21,211 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 0
2019-03-20 15:52:21,222 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 0 secs
2019-03-20 15:52:21,222 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 0 racks and 0 datanodes
2019-03-20 15:52:21,222 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-20 15:52:21,256 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-20 15:52:21,256 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-20 15:52:21,259 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-20 15:52:21,262 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-20 15:52:21,269 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-20 15:52:21,275 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-20 15:52:21,769 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 15:52:21,772 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.6:9866
2019-03-20 15:52:21,774 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 4ea34f12-a71f-4771-a263-13486b1812f9 (10.10.1.6:9866).
2019-03-20 15:52:21,776 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 15:52:21,776 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.3:9866
2019-03-20 15:52:21,777 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN e77c7a7a-c22c-4855-abad-bc8542b928ae (10.10.1.3:9866).
2019-03-20 15:52:21,815 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 15:52:21,816 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.2:9866
2019-03-20 15:52:21,816 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN cc2f05d0-e01b-494e-bed6-0c53648ffd86 (10.10.1.2:9866).
2019-03-20 15:52:21,859 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 for DN 10.10.1.6:9866
2019-03-20 15:52:21,863 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-adde346a-0c8d-4bfe-a820-c69f66587cbd for DN 10.10.1.3:9866
2019-03-20 15:52:21,866 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-0832497a-e5df-407b-a464-33936c781e51 for DN 10.10.1.2:9866
2019-03-20 15:52:21,912 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x58b8c7d6663ea4cc: Processing first storage report for DS-adde346a-0c8d-4bfe-a820-c69f66587cbd from datanode e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 15:52:21,914 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x58b8c7d6663ea4cc: from storage DS-adde346a-0c8d-4bfe-a820-c69f66587cbd node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-03-20 15:52:21,914 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x66a8e2582602773: Processing first storage report for DS-0832497a-e5df-407b-a464-33936c781e51 from datanode cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 15:52:21,915 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x66a8e2582602773: from storage DS-0832497a-e5df-407b-a464-33936c781e51 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 15:52:21,915 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xc665bbb918b3feb8: Processing first storage report for DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 from datanode 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 15:52:21,915 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xc665bbb918b3feb8: from storage DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 node DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 15:52:22,073 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 15:52:22,073 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.5:9866
2019-03-20 15:52:22,073 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 42f6c731-38e3-48aa-863c-f30542c9be35 (10.10.1.5:9866).
2019-03-20 15:52:22,114 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe for DN 10.10.1.5:9866
2019-03-20 15:52:22,138 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xd67e907b21d8237c: Processing first storage report for DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe from datanode 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 15:52:22,138 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xd67e907b21d8237c: from storage DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 15:52:22,458 INFO  namenode.FSNamesystem FSNamesystem.java:stopStandbyServices:1408 - Stopping services started for standby state
2019-03-20 15:52:22,459 WARN  ha.EditLogTailer EditLogTailer.java:doWork:481 - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-03-20 15:52:22,563 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1203 - Starting services required for active state
2019-03-20 15:52:22,571 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:435 - Starting recovery process for unclosed journal segments...
2019-03-20 15:52:22,658 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:437 - Successfully started new epoch 1
2019-03-20 15:52:22,658 INFO  namenode.FileJournalManager FileJournalManager.java:recoverUnfinalizedSegments:396 - Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-03-20 15:52:22,658 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1214 - Catching up to latest edits from old active before taking over writer role in edits logs
2019-03-20 15:52:22,662 INFO  blockmanagement.DatanodeManager DatanodeManager.java:markAllDatanodesStale:1837 - Marking all datanodes as stale
2019-03-20 15:52:22,664 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1225 - Reprocessing replication and invalidation queues
2019-03-20 15:52:22,665 INFO  blockmanagement.BlockManager BlockManager.java:initializeReplQueues:4752 - initializing replication queues
2019-03-20 15:52:22,665 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1236 - Will take over writing edit logs at txnid 1
2019-03-20 15:52:22,668 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 1
2019-03-20 15:52:23,119 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:767 - Initializing quota with 4 thread(s)
2019-03-20 15:52:23,124 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:776 - Quota initialization completed in 5 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-03-20 15:52:23,129 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3447 - Total number of blocks            = 0
2019-03-20 15:52:23,129 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3448 - Number of invalid blocks          = 0
2019-03-20 15:52:23,129 INFO  blockmanagement.CacheReplicationMonitor CacheReplicationMonitor.java:run:160 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-03-20 15:52:23,130 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3449 - Number of under-replicated blocks = 0
2019-03-20 15:52:23,130 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3450 - Number of  over-replicated blocks = 0
2019-03-20 15:52:23,130 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3452 - Number of blocks being written    = 0
2019-03-20 15:52:23,130 INFO  hdfs.StateChange BlockManager.java:processMisReplicatesAsync:3455 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 465 msec
2019-03-20 15:52:28,886 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741825_1001, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 15:52:28,890 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741826_1002, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:52:28,890 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741827_1003, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:52:28,891 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741828_1004, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:52:28,893 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741829_1005, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:52:29,058 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741830_1006, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 15:52:29,073 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741831_1007, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:52:29,074 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741832_1008, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:52:29,074 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741833_1009, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 15:52:29,075 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741834_1010, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:52:29,105 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,106 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,106 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,106 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741835_1011, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 15:52:29,113 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,114 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,114 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,114 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741836_1012, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:52:29,122 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,123 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,123 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,124 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741837_1013, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:52:29,138 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,139 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,139 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,139 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741838_1014, replicas=10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:52:29,141 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,141 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,142 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,142 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741839_1015, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 15:52:29,143 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,143 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,143 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,144 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741840_1016, replicas=10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 15:52:29,151 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,152 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,152 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,152 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741841_1017, replicas=10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:52:29,160 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,160 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,160 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,160 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741842_1018, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:52:29,168 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,168 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,168 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,168 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741843_1019, replicas=10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:52:29,184 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:52:29,185 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:52:29,185 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:52:29,185 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741844_1020, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 15:53:13,388 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 92 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 66 Number of syncs: 25 SyncTimes(ms): 198 214 
2019-03-20 15:53:13,450 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741845_1021, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 15:53:13,626 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741846_1022, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 15:53:13,674 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741847_1023, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:53:13,678 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741848_1024, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:53:13,725 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:13,725 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:13,725 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:13,726 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741849_1025, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:53:13,741 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741850_1026, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 15:53:13,846 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:13,846 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:13,846 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:13,847 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741851_1027, replicas=10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:53:13,876 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741852_1028, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:53:13,877 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741853_1029, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:53:13,907 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741854_1030, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 15:53:13,960 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741855_1031, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:53:14,042 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:14,042 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:14,042 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:14,042 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:14,042 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:14,042 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:14,043 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741856_1032, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:53:14,043 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741857_1033, replicas=10.10.1.6:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 15:53:14,124 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:14,124 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:14,125 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:14,125 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741858_1034, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:53:14,125 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741859_1035, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:53:14,204 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:14,204 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:14,204 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:14,204 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741860_1036, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:53:14,212 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:14,212 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:14,213 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:14,213 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741861_1037, replicas=10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 15:53:14,228 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:14,229 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:14,229 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:14,229 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741862_1038, replicas=10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:53:14,232 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:14,233 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:14,233 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:14,233 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741863_1039, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:53:14,312 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:14,312 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:14,312 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:14,312 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741864_1040, replicas=10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:53:57,544 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741865_1041, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:53:57,635 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741866_1042, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:53:57,704 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741867_1043, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 15:53:57,769 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:57,770 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:57,770 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:57,770 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741868_1044, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:53:57,786 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741869_1045, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 15:53:57,832 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:57,832 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:57,832 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:57,833 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741870_1046, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:53:57,853 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:57,853 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:57,853 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:57,853 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741871_1047, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 15:53:57,915 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:57,916 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:57,916 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:57,916 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741872_1048, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 15:53:58,795 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741873_1049, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:53:58,797 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741874_1050, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:53:58,868 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741875_1051, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:53:58,962 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741876_1052, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:53:58,979 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741877_1053, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:53:59,028 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741878_1054, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:53:59,029 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:59,029 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:59,029 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:59,030 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741879_1055, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:53:59,044 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:59,044 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:59,044 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:59,045 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741880_1056, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:53:59,090 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:59,091 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:59,091 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:59,091 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741881_1057, replicas=10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:53:59,094 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:59,094 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:59,095 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:59,095 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741882_1058, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:53:59,107 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:59,107 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:59,107 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:59,108 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741883_1059, replicas=10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:53:59,156 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:53:59,156 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:53:59,157 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:53:59,157 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741884_1060, replicas=10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:54:21,338 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 15:54:21,339 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 15:54:21,339 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 1, 271
2019-03-20 15:54:21,339 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 272 Total time for transactions(ms): 26 Number of transactions batched in Syncs: 169 Number of syncs: 102 SyncTimes(ms): 1369 732 
2019-03-20 15:54:21,360 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 272 Total time for transactions(ms): 26 Number of transactions batched in Syncs: 169 Number of syncs: 103 SyncTimes(ms): 1384 737 
2019-03-20 15:54:21,393 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000001-0000000000000000272
2019-03-20 15:54:21,410 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 273
2019-03-20 15:54:41,352 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741885_1061, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:54:41,497 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741886_1062, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:54:41,539 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:41,539 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:41,539 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:41,540 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741887_1063, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:54:41,576 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:41,576 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:41,576 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:41,576 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741888_1064, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:54:41,642 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741889_1065, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:54:41,789 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741890_1066, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 15:54:41,829 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:41,830 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:41,830 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:41,830 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741891_1067, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 15:54:41,867 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:41,868 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:41,868 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:41,868 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741892_1068, replicas=10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 15:54:43,487 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741893_1069, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:54:43,494 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741894_1070, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:54:43,496 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741895_1071, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:54:43,630 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741896_1072, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:54:43,638 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741897_1073, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:54:43,639 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741898_1074, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:54:43,663 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:43,663 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:43,663 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:43,663 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741899_1075, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:54:43,670 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:43,671 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:43,671 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:43,671 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:43,671 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:43,671 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:43,671 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741900_1076, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:54:43,672 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741901_1077, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:54:43,691 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:43,692 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:43,692 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:43,692 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741902_1078, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:54:43,700 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:43,700 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:54:43,700 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:43,700 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:54:43,700 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:43,700 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:54:43,701 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741903_1079, replicas=10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:54:43,701 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741904_1080, replicas=10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:55:25,180 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 92 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 41 Number of syncs: 50 SyncTimes(ms): 306 262 
2019-03-20 15:55:25,246 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741905_1081, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 15:55:25,400 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741906_1082, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:55:25,441 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:25,441 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:25,441 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:25,442 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741907_1083, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:55:25,479 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:25,479 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:25,479 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:25,480 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741908_1084, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:55:25,505 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741909_1085, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 15:55:25,642 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741910_1086, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 15:55:25,691 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:25,691 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:25,691 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:25,692 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741911_1087, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 15:55:25,728 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:25,729 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:25,729 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:25,729 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741912_1088, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 15:55:27,484 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741913_1089, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:55:27,572 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741914_1090, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:55:27,658 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741915_1091, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:55:27,681 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741916_1092, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 15:55:27,707 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:27,707 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:27,707 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:27,708 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741917_1093, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:55:27,733 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741918_1094, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:55:27,753 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:27,753 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:27,753 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:27,753 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741919_1095, replicas=10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:55:27,782 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:27,782 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:27,782 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:27,783 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741920_1096, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:55:27,811 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:27,811 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:27,811 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:27,812 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741921_1097, replicas=10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:55:27,824 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741922_1098, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 15:55:27,856 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:27,856 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:27,856 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:27,857 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741923_1099, replicas=10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 15:55:27,894 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:55:27,894 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:55:27,894 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:55:27,895 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741924_1100, replicas=10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 15:56:08,975 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741925_1101, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:56:09,120 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741926_1102, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:56:09,161 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:09,161 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:09,161 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:09,162 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741927_1103, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:56:09,199 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:09,199 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:09,199 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:09,199 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741928_1104, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:56:09,433 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741929_1105, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 15:56:09,570 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741930_1106, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:56:09,610 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:09,610 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:09,611 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:09,611 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741931_1107, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:56:09,648 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:09,648 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:09,649 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:09,649 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741932_1108, replicas=10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:56:11,388 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741933_1109, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:56:11,517 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741934_1110, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 15:56:11,536 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741935_1111, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:56:11,569 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:11,570 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:11,570 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:11,570 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741936_1112, replicas=10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:56:11,615 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:11,615 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:11,615 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:11,616 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741937_1113, replicas=10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:56:11,679 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741938_1114, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:56:11,718 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:11,718 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:11,718 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:11,718 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741939_1115, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:56:11,755 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:11,756 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:11,756 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:11,756 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741940_1116, replicas=10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 15:56:11,843 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741941_1117, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:56:12,011 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741942_1118, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 15:56:12,052 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:12,052 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:12,052 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:12,052 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741943_1119, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 15:56:12,089 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:12,089 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:12,089 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:12,089 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741944_1120, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 15:56:21,796 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 15:56:21,796 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 15:56:21,796 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 273, 543
2019-03-20 15:56:21,824 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 272 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 92 Number of syncs: 180 SyncTimes(ms): 1205 744 
2019-03-20 15:56:21,827 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000273 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000273-0000000000000000544
2019-03-20 15:56:21,827 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 545
2019-03-20 15:56:52,605 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741945_1121, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:56:52,752 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741946_1122, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 15:56:52,809 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:52,810 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:52,810 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:52,810 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741947_1123, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 15:56:52,855 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:52,855 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:52,856 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:52,856 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741948_1124, replicas=10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 15:56:53,332 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741949_1125, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 15:56:53,494 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741950_1126, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:56:53,559 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:53,559 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:53,560 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:53,560 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741951_1127, replicas=10.10.1.6:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:56:53,605 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:53,605 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:53,606 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:53,606 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741952_1128, replicas=10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:56:54,859 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741953_1129, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 15:56:55,018 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741954_1130, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 15:56:55,075 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:55,076 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:55,076 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:55,076 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741955_1131, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 15:56:55,121 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:55,121 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:55,121 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:55,122 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741956_1132, replicas=10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 15:56:55,209 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741957_1133, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 15:56:55,359 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741958_1134, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 15:56:55,425 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:55,425 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:55,425 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:55,425 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741959_1135, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:56:55,470 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:55,471 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:55,471 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:55,471 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741960_1136, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:56:55,922 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741961_1137, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 15:56:56,077 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741962_1138, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:56:56,141 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:56,141 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:56,142 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:56,142 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741963_1139, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 15:56:56,187 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:56:56,187 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:56:56,187 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:56:56,187 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741964_1140, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 15:57:26,695 ERROR namenode.NameNode LogAdapter.java:error:75 - RECEIVED SIGNAL 15: SIGTERM
2019-03-20 15:57:26,697 INFO  namenode.NameNode LogAdapter.java:info:51 - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode059.clemson.cloudlab.us/130.127.133.68
************************************************************/
2019-03-20 15:57:30,840 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-20 15:57:30,852 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-20 15:57:30,857 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-20 15:57:31,099 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-20 15:57:31,142 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-20 15:57:31,245 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-20 15:57:31,246 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-20 15:57:31,298 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-20 15:57:31,298 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-20 15:57:31,463 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-20 15:57:31,490 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-20 15:57:31,508 INFO  util.log Log.java:initialized:192 - Logging initialized @1190ms
2019-03-20 15:57:31,615 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-20 15:57:31,627 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-20 15:57:31,638 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-20 15:57:31,641 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-20 15:57:31,642 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-20 15:57:31,643 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-20 15:57:31,669 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-20 15:57:31,669 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-20 15:57:31,678 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-20 15:57:31,680 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-20 15:57:31,716 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-20 15:57:31,717 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-20 15:57:31,792 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-20 15:57:31,815 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-20 15:57:31,815 INFO  server.Server Server.java:doStart:414 - Started @1499ms
2019-03-20 15:57:32,195 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-20 15:57:32,248 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-20 15:57:32,263 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-20 15:57:32,265 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-20 15:57:32,267 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-20 15:57:32,274 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-20 15:57:32,274 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-20 15:57:32,274 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-20 15:57:32,275 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-20 15:57:32,275 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-20 15:57:32,318 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-20 15:57:32,330 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-20 15:57:32,330 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-20 15:57:32,335 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-20 15:57:32,335 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 20 15:57:32
2019-03-20 15:57:32,338 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-20 15:57:32,338 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:57:32,339 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-20 15:57:32,340 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-20 15:57:32,481 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-20 15:57:32,491 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-20 15:57:32,491 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-20 15:57:32,491 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-20 15:57:32,491 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-20 15:57:32,491 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-20 15:57:32,492 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-20 15:57:32,492 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-20 15:57:32,492 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-20 15:57:32,492 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-20 15:57:32,492 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = true
2019-03-20 15:57:32,492 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-20 15:57:32,575 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-20 15:57:32,576 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:57:32,576 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-20 15:57:32,576 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-20 15:57:32,649 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-20 15:57:32,649 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-20 15:57:32,650 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-20 15:57:32,650 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-20 15:57:32,657 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-20 15:57:32,659 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-20 15:57:32,665 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-20 15:57:32,665 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:57:32,665 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-20 15:57:32,665 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-20 15:57:32,693 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-20 15:57:32,693 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-20 15:57:32,693 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-20 15:57:32,698 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-20 15:57:32,698 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-20 15:57:32,700 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-20 15:57:32,700 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 15:57:32,701 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-20 15:57:32,701 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-20 15:57:32,769 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 119314@clnode059.clemson.cloudlab.us
2019-03-20 15:57:34,121 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-20 15:57:34,184 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-20 15:57:34,186 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-20 15:57:34,217 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-20 15:57:34,218 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-20 15:57:34,223 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #1
2019-03-20 15:57:34,223 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 15:57:34,227 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 15:57:34,227 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 15:57:34,332 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12611 edits # 272 loaded in 0 seconds
2019-03-20 15:57:34,332 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4044fb95 expecting start txid #273
2019-03-20 15:57:34,332 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 15:57:34,332 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 15:57:34,333 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 15:57:34,356 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12613 edits # 272 loaded in 0 seconds
2019-03-20 15:57:34,356 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #545
2019-03-20 15:57:34,356 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 15:57:34,356 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 15:57:34,356 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 15:57:34,368 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 15:57:34,368 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-20 15:57:34,369 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 0 entries 0 lookups
2019-03-20 15:57:34,369 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 1660 msecs
2019-03-20 15:57:34,555 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-20 15:57:34,560 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-20 15:57:34,573 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-20 15:57:34,759 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-20 15:57:34,769 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 0
2019-03-20 15:57:34,779 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 0 secs
2019-03-20 15:57:34,780 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 0 racks and 0 datanodes
2019-03-20 15:57:34,780 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-20 15:57:34,819 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-20 15:57:34,819 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-20 15:57:34,823 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-20 15:57:34,827 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-20 15:57:34,834 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-20 15:57:34,840 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-20 15:57:35,512 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 15:57:35,513 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.6:9866
2019-03-20 15:57:35,514 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 4ea34f12-a71f-4771-a263-13486b1812f9 (10.10.1.6:9866).
2019-03-20 15:57:35,516 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 15:57:35,516 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.5:9866
2019-03-20 15:57:35,516 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 42f6c731-38e3-48aa-863c-f30542c9be35 (10.10.1.5:9866).
2019-03-20 15:57:35,517 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 15:57:35,517 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.3:9866
2019-03-20 15:57:35,517 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN e77c7a7a-c22c-4855-abad-bc8542b928ae (10.10.1.3:9866).
2019-03-20 15:57:35,519 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 15:57:35,519 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.2:9866
2019-03-20 15:57:35,519 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN cc2f05d0-e01b-494e-bed6-0c53648ffd86 (10.10.1.2:9866).
2019-03-20 15:57:35,529 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe for DN 10.10.1.5:9866
2019-03-20 15:57:35,531 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-0832497a-e5df-407b-a464-33936c781e51 for DN 10.10.1.2:9866
2019-03-20 15:57:35,532 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-adde346a-0c8d-4bfe-a820-c69f66587cbd for DN 10.10.1.3:9866
2019-03-20 15:57:35,532 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 for DN 10.10.1.6:9866
2019-03-20 15:57:35,544 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xd67e907b21d8237d: Processing first storage report for DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe from datanode 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 15:57:35,545 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xd67e907b21d8237d: from storage DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2019-03-20 15:57:35,545 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x58b8c7d6663ea4cd: Processing first storage report for DS-adde346a-0c8d-4bfe-a820-c69f66587cbd from datanode e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 15:57:35,546 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x58b8c7d6663ea4cd: from storage DS-adde346a-0c8d-4bfe-a820-c69f66587cbd node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 15:57:35,546 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xc665bbb918b3feb9: Processing first storage report for DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 from datanode 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 15:57:35,546 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xc665bbb918b3feb9: from storage DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 node DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 15:57:35,546 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x66a8e2582602774: Processing first storage report for DS-0832497a-e5df-407b-a464-33936c781e51 from datanode cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 15:57:35,546 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x66a8e2582602774: from storage DS-0832497a-e5df-407b-a464-33936c781e51 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 15:57:54,752 INFO  namenode.FSNamesystem FSNamesystem.java:stopStandbyServices:1408 - Stopping services started for standby state
2019-03-20 15:57:54,753 WARN  ha.EditLogTailer EditLogTailer.java:doWork:481 - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-03-20 15:57:54,756 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1203 - Starting services required for active state
2019-03-20 15:57:54,763 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:435 - Starting recovery process for unclosed journal segments...
2019-03-20 15:57:54,814 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:437 - Successfully started new epoch 3
2019-03-20 15:57:54,814 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:277 - Beginning recovery of unclosed segment starting at txid 636
2019-03-20 15:57:54,834 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:286 - Recovery prepare phase complete. Responses:
10.10.1.5:8485: segmentState { startTxId: 636 endTxId: 726 isInProgress: true } lastWriterEpoch: 2 lastCommittedTxId: 725
10.10.1.2:8485: segmentState { startTxId: 636 endTxId: 726 isInProgress: true } lastWriterEpoch: 2 lastCommittedTxId: 725
2019-03-20 15:57:54,836 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:310 - Using longest log: 10.10.1.5:8485=segmentState {
  startTxId: 636
  endTxId: 726
  isInProgress: true
}
lastWriterEpoch: 2
lastCommittedTxId: 725

2019-03-20 15:57:54,873 INFO  namenode.FileJournalManager FileJournalManager.java:recoverUnfinalizedSegments:396 - Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-03-20 15:57:54,902 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000545 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000545-0000000000000000635
2019-03-20 15:57:54,918 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1214 - Catching up to latest edits from old active before taking over writer role in edits logs
2019-03-20 15:57:54,923 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1fd6f3be expecting start txid #636
2019-03-20 15:57:54,923 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 15:57:54,924 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 636
2019-03-20 15:57:54,924 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 636
2019-03-20 15:57:54,939 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 15:57:54,939 INFO  blockmanagement.DatanodeManager DatanodeManager.java:markAllDatanodesStale:1837 - Marking all datanodes as stale
2019-03-20 15:57:54,940 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1225 - Reprocessing replication and invalidation queues
2019-03-20 15:57:54,940 INFO  blockmanagement.BlockManager BlockManager.java:initializeReplQueues:4752 - initializing replication queues
2019-03-20 15:57:54,941 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1236 - Will take over writing edit logs at txnid 727
2019-03-20 15:57:54,943 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 727
2019-03-20 15:57:55,213 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:767 - Initializing quota with 4 thread(s)
2019-03-20 15:57:55,217 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:776 - Quota initialization completed in 3 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-03-20 15:57:55,223 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3447 - Total number of blocks            = 0
2019-03-20 15:57:55,223 INFO  blockmanagement.CacheReplicationMonitor CacheReplicationMonitor.java:run:160 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-03-20 15:57:55,223 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3448 - Number of invalid blocks          = 0
2019-03-20 15:57:55,223 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3449 - Number of under-replicated blocks = 0
2019-03-20 15:57:55,223 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3450 - Number of  over-replicated blocks = 0
2019-03-20 15:57:55,223 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3452 - Number of blocks being written    = 0
2019-03-20 15:57:55,223 INFO  hdfs.StateChange BlockManager.java:processMisReplicatesAsync:3455 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 283 msec
2019-03-20 15:58:23,988 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741985_1161, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:58:24,181 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741986_1162, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:58:24,249 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:24,249 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:24,249 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:24,250 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741987_1163, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:58:24,309 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:24,310 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:24,310 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:24,310 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741988_1164, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:58:27,196 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741989_1165, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:58:27,363 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741990_1166, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:58:27,431 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:27,431 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:27,431 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:27,432 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741991_1167, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:58:27,491 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:27,492 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:27,492 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:27,492 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741992_1168, replicas=10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:58:28,838 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741993_1169, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 15:58:29,021 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741994_1170, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 15:58:29,088 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:29,088 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:29,089 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:29,089 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741995_1171, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 15:58:29,148 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:29,149 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:29,149 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:29,149 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741996_1172, replicas=10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 15:58:32,276 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741997_1173, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:58:32,276 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 58 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 738 Number of syncs: 44 SyncTimes(ms): 385 463 
2019-03-20 15:58:32,462 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741998_1174, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 15:58:32,526 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:32,526 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:32,527 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:32,527 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073741999_1175, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 15:58:32,589 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:32,589 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:32,589 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:32,590 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742000_1176, replicas=10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 15:58:34,142 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742001_1177, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:58:34,301 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742002_1178, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:58:34,367 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:34,367 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:34,367 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:34,368 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742003_1179, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:58:34,429 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:58:34,430 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:58:34,430 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:58:34,430 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742004_1180, replicas=10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 15:59:08,201 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742005_1181, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 15:59:08,376 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742006_1182, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 15:59:08,441 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:08,442 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:08,442 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:08,442 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742007_1183, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:59:08,496 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:08,496 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:08,496 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:08,496 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742008_1184, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:59:11,239 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742009_1185, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:59:11,408 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742010_1186, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:59:11,474 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:11,474 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:11,475 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:11,475 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742011_1187, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:59:11,528 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:11,528 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:11,528 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:11,528 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742012_1188, replicas=10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 15:59:12,820 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742013_1189, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 15:59:12,973 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742014_1190, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:59:13,040 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:13,040 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:13,040 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:13,041 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742015_1191, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:59:13,093 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:13,094 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:13,094 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:13,094 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742016_1192, replicas=10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:59:16,171 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742017_1193, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 15:59:16,339 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742018_1194, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:59:16,404 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:16,404 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:16,404 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:16,405 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742019_1195, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:59:16,450 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:16,450 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:16,450 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:16,451 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742020_1196, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 15:59:17,934 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742021_1197, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:59:18,087 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742022_1198, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 15:59:18,153 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:18,153 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:18,153 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:18,154 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742023_1199, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 15:59:18,216 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:18,216 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:18,216 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:18,217 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742024_1200, replicas=10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 15:59:52,232 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 182 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 766 Number of syncs: 141 SyncTimes(ms): 1211 1274 
2019-03-20 15:59:52,304 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742025_1201, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 15:59:52,445 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742026_1202, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 15:59:52,494 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:52,494 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:52,494 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:52,495 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742027_1203, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:59:52,532 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:52,532 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:52,532 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:52,533 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742028_1204, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 15:59:54,928 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742029_1205, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 15:59:55,094 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742030_1206, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 15:59:55,143 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:55,143 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:55,144 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:55,144 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742031_1207, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 15:59:55,180 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:55,181 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:55,181 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:55,181 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742032_1208, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 15:59:56,640 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742033_1209, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 15:59:56,793 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742034_1210, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:59:56,834 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:56,834 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:56,834 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:56,835 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742035_1211, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 15:59:56,871 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 15:59:56,872 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 15:59:56,872 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 15:59:56,872 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742036_1212, replicas=10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:00:00,264 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742037_1213, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:00:00,400 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742038_1214, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:00:00,449 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:00,449 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:00,449 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:00,449 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742039_1215, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:00:00,486 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:00,487 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:00,487 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:00,487 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742040_1216, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:00:01,796 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742041_1217, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:00:01,941 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742042_1218, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:00:01,991 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:01,992 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:01,992 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:01,992 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742043_1219, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:00:02,027 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:02,027 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:02,028 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:02,028 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742044_1220, replicas=10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:00:04,427 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 16:00:04,428 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 16:00:04,428 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 727, 997
2019-03-20 16:00:04,449 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 272 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 786 Number of syncs: 212 SyncTimes(ms): 1721 1619 
2019-03-20 16:00:04,453 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000727 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000727-0000000000000000998
2019-03-20 16:00:04,453 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 999
2019-03-20 16:00:36,275 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742045_1221, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:00:36,409 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742046_1222, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:00:36,450 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:36,450 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:36,450 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:36,451 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742047_1223, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:00:36,488 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:36,488 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:36,488 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:36,488 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742048_1224, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:00:38,937 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742049_1225, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:00:39,083 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742050_1226, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:00:39,135 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:39,135 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:39,135 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:39,135 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742051_1227, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:00:39,170 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:39,170 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:39,170 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:39,170 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742052_1228, replicas=10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:00:40,382 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742053_1229, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:00:40,532 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742054_1230, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:00:40,573 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:40,573 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:40,574 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:40,574 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742055_1231, replicas=10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:00:40,602 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:40,603 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:40,603 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:40,603 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742056_1232, replicas=10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:00:44,104 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742057_1233, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:00:44,281 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742058_1234, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:00:44,321 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:44,321 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:44,321 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:44,322 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742059_1235, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:00:44,359 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:44,359 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:44,359 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:44,360 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742060_1236, replicas=10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:00:45,481 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742061_1237, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:00:45,613 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742062_1238, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:00:45,654 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:45,654 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:45,654 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:45,654 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742063_1239, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:00:45,691 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:00:45,692 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:00:45,692 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:00:45,692 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742064_1240, replicas=10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:01:19,889 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 92 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 21 Number of syncs: 70 SyncTimes(ms): 471 392 
2019-03-20 16:01:19,954 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742065_1241, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:01:20,096 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742066_1242, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:01:20,137 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:20,137 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:20,138 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:20,138 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742067_1243, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:01:20,174 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:20,175 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:20,175 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:20,175 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742068_1244, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:01:22,526 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742069_1245, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:01:22,661 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742070_1246, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:01:22,702 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:22,702 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:22,703 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:22,703 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742071_1247, replicas=10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:01:22,739 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:22,740 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:22,740 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:22,740 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742072_1248, replicas=10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:01:24,038 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742073_1249, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:01:24,177 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742074_1250, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:01:24,218 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:24,218 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:24,218 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:24,218 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742075_1251, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:01:24,255 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:24,256 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:24,256 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:24,256 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742076_1252, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:01:27,890 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742077_1253, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:01:28,025 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742078_1254, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:01:28,066 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:28,066 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:28,066 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:28,067 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742079_1255, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:01:28,104 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:28,104 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:28,104 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:28,104 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742080_1256, replicas=10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:01:29,308 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742081_1257, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:01:29,450 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742082_1258, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:01:29,490 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:29,490 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:29,490 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:29,491 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742083_1259, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:01:29,528 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:01:29,528 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:01:29,529 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:01:29,529 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742084_1260, replicas=10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:02:03,948 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742085_1261, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:02:04,108 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742086_1262, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:02:04,148 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:04,148 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:04,148 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:04,149 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742087_1263, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:02:04,186 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:04,186 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:04,186 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:04,186 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742088_1264, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:02:04,642 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 16:02:04,642 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 16:02:04,642 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 999, 1197
2019-03-20 16:02:04,650 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 200 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 45 Number of syncs: 155 SyncTimes(ms): 1064 696 
2019-03-20 16:02:04,653 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000000999 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000000999-0000000000000001198
2019-03-20 16:02:04,653 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 1199
2019-03-20 16:02:06,578 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742089_1265, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:02:06,737 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742090_1266, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:02:06,805 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:06,805 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:06,806 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:06,806 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742091_1267, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:02:06,857 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:06,857 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:06,858 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:06,858 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742092_1268, replicas=10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:02:07,966 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742093_1269, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:02:08,129 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742094_1270, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:02:08,194 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:08,195 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:08,195 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:08,195 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742095_1271, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:02:08,251 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:08,252 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:08,252 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:08,252 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742096_1272, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:02:11,711 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742097_1273, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:02:11,877 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742098_1274, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:02:11,943 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:11,943 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:11,943 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:11,944 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742099_1275, replicas=10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:02:11,996 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:11,997 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:11,997 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:11,997 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742100_1276, replicas=10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:02:13,373 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742101_1277, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:02:13,525 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742102_1278, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:02:13,591 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:13,591 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:13,591 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:13,592 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742103_1279, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:02:13,637 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:13,637 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:13,638 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:13,638 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742104_1280, replicas=10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:02:47,872 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742105_1281, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:02:48,033 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742106_1282, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:02:48,099 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:48,099 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:48,099 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:48,100 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742107_1283, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:02:48,153 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:48,154 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:48,154 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:48,154 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742108_1284, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:02:50,413 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742109_1285, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:02:50,607 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742110_1286, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:02:50,674 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:50,674 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:50,674 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:50,674 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742111_1287, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:02:50,727 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:50,728 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:50,728 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:50,728 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742112_1288, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:02:51,842 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742113_1289, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:02:51,998 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742114_1290, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:02:52,064 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:52,064 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:52,064 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:52,064 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742115_1291, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:02:52,118 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:52,118 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:52,118 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:52,119 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742116_1292, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:02:55,513 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742117_1293, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:02:55,681 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742118_1294, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:02:55,745 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:55,745 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:55,746 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:55,746 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742119_1295, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:02:55,799 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:55,800 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:55,800 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:55,800 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742120_1296, replicas=10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:02:56,760 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742121_1297, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:02:56,939 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742122_1298, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:02:57,003 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:57,003 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:57,004 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:57,004 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742123_1299, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:02:57,058 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:02:57,058 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:02:57,058 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:02:57,058 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742124_1300, replicas=10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:03:11,511 ERROR namenode.NameNode LogAdapter.java:error:75 - RECEIVED SIGNAL 15: SIGTERM
2019-03-20 16:03:11,514 INFO  namenode.NameNode LogAdapter.java:info:51 - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode059.clemson.cloudlab.us/130.127.133.68
************************************************************/
2019-03-20 16:03:15,614 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-20 16:03:15,626 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-20 16:03:15,631 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-20 16:03:15,876 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-20 16:03:15,920 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-20 16:03:16,025 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-20 16:03:16,026 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-20 16:03:16,077 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-20 16:03:16,078 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-20 16:03:16,239 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-20 16:03:16,267 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-20 16:03:16,285 INFO  util.log Log.java:initialized:192 - Logging initialized @1180ms
2019-03-20 16:03:16,395 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-20 16:03:16,407 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-20 16:03:16,418 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-20 16:03:16,421 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-20 16:03:16,422 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-20 16:03:16,423 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-20 16:03:16,449 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-20 16:03:16,449 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-20 16:03:16,459 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-20 16:03:16,460 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-20 16:03:16,495 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-20 16:03:16,496 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-20 16:03:16,571 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-20 16:03:16,595 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-20 16:03:16,595 INFO  server.Server Server.java:doStart:414 - Started @1492ms
2019-03-20 16:03:16,935 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-20 16:03:16,986 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-20 16:03:17,000 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-20 16:03:17,002 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-20 16:03:17,005 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-20 16:03:17,011 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-20 16:03:17,011 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-20 16:03:17,011 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-20 16:03:17,012 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-20 16:03:17,012 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-20 16:03:17,054 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-20 16:03:17,067 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-20 16:03:17,067 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-20 16:03:17,072 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-20 16:03:17,072 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 20 16:03:17
2019-03-20 16:03:17,074 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-20 16:03:17,074 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:03:17,076 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-20 16:03:17,076 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-20 16:03:17,218 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-20 16:03:17,227 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-20 16:03:17,227 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-20 16:03:17,228 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-20 16:03:17,228 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-20 16:03:17,228 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-20 16:03:17,228 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-20 16:03:17,228 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-20 16:03:17,228 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-20 16:03:17,228 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-20 16:03:17,229 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = true
2019-03-20 16:03:17,229 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-20 16:03:17,311 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-20 16:03:17,311 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:03:17,312 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-20 16:03:17,312 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-20 16:03:17,385 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-20 16:03:17,385 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-20 16:03:17,385 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-20 16:03:17,386 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-20 16:03:17,392 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-20 16:03:17,395 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-20 16:03:17,400 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-20 16:03:17,401 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:03:17,401 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-20 16:03:17,401 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-20 16:03:17,428 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-20 16:03:17,428 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-20 16:03:17,429 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-20 16:03:17,433 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-20 16:03:17,433 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-20 16:03:17,435 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-20 16:03:17,435 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:03:17,436 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-20 16:03:17,436 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-20 16:03:17,505 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 121021@clnode059.clemson.cloudlab.us
2019-03-20 16:03:18,940 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 1197ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=335ms
GC pool 'PS Scavenge' had collection(s): count=1 time=971ms
2019-03-20 16:03:19,109 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-20 16:03:19,171 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-20 16:03:19,173 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-20 16:03:19,204 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-20 16:03:19,204 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-20 16:03:19,209 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #1
2019-03-20 16:03:19,209 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:03:19,213 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,213 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,319 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12611 edits # 272 loaded in 0 seconds
2019-03-20 16:03:19,319 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4044fb95 expecting start txid #273
2019-03-20 16:03:19,319 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:03:19,319 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,319 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,341 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12613 edits # 272 loaded in 0 seconds
2019-03-20 16:03:19,341 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #545
2019-03-20 16:03:19,341 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:03:19,341 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,341 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,353 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 16:03:19,353 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@36f48b4 expecting start txid #636
2019-03-20 16:03:19,353 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:03:19,353 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,354 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,363 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 16:03:19,364 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5c00384f expecting start txid #727
2019-03-20 16:03:19,364 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:03:19,364 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,364 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,382 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12612 edits # 272 loaded in 0 seconds
2019-03-20 16:03:19,382 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3b7ff809 expecting start txid #999
2019-03-20 16:03:19,382 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:03:19,382 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,383 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,394 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 9258 edits # 200 loaded in 0 seconds
2019-03-20 16:03:19,394 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1bb564e2 expecting start txid #1199
2019-03-20 16:03:19,395 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:03:19,395 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,395 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:03:19,405 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 163 loaded in 0 seconds
2019-03-20 16:03:19,405 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-20 16:03:19,405 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 5 entries 75 lookups
2019-03-20 16:03:19,405 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 1962 msecs
2019-03-20 16:03:19,591 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-20 16:03:19,597 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-20 16:03:19,610 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-20 16:03:19,805 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-20 16:03:19,815 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 0
2019-03-20 16:03:19,827 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 0 secs
2019-03-20 16:03:19,827 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 0 racks and 0 datanodes
2019-03-20 16:03:19,827 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-20 16:03:19,859 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-20 16:03:19,860 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-20 16:03:19,862 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-20 16:03:19,865 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-20 16:03:19,870 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-20 16:03:19,876 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-20 16:03:20,666 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 16:03:20,667 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.5:9866
2019-03-20 16:03:20,668 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 42f6c731-38e3-48aa-863c-f30542c9be35 (10.10.1.5:9866).
2019-03-20 16:03:20,670 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 16:03:20,670 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.3:9866
2019-03-20 16:03:20,670 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN e77c7a7a-c22c-4855-abad-bc8542b928ae (10.10.1.3:9866).
2019-03-20 16:03:20,671 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 16:03:20,671 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.6:9866
2019-03-20 16:03:20,671 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 4ea34f12-a71f-4771-a263-13486b1812f9 (10.10.1.6:9866).
2019-03-20 16:03:20,672 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 16:03:20,672 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.2:9866
2019-03-20 16:03:20,672 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN cc2f05d0-e01b-494e-bed6-0c53648ffd86 (10.10.1.2:9866).
2019-03-20 16:03:20,681 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 for DN 10.10.1.6:9866
2019-03-20 16:03:20,684 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-0832497a-e5df-407b-a464-33936c781e51 for DN 10.10.1.2:9866
2019-03-20 16:03:20,684 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe for DN 10.10.1.5:9866
2019-03-20 16:03:20,685 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-adde346a-0c8d-4bfe-a820-c69f66587cbd for DN 10.10.1.3:9866
2019-03-20 16:03:20,696 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xd67e907b21d8237e: Processing first storage report for DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe from datanode 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 16:03:20,698 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xd67e907b21d8237e: from storage DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2019-03-20 16:03:20,698 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x58b8c7d6663ea4ce: Processing first storage report for DS-adde346a-0c8d-4bfe-a820-c69f66587cbd from datanode e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 16:03:20,698 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x58b8c7d6663ea4ce: from storage DS-adde346a-0c8d-4bfe-a820-c69f66587cbd node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:03:20,698 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xc665bbb918b3feba: Processing first storage report for DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 from datanode 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 16:03:20,698 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xc665bbb918b3feba: from storage DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 node DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:03:20,698 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x66a8e2582602775: Processing first storage report for DS-0832497a-e5df-407b-a464-33936c781e51 from datanode cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 16:03:20,699 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x66a8e2582602775: from storage DS-0832497a-e5df-407b-a464-33936c781e51 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:03:39,211 INFO  namenode.FSNamesystem FSNamesystem.java:stopStandbyServices:1408 - Stopping services started for standby state
2019-03-20 16:03:39,212 WARN  ha.EditLogTailer EditLogTailer.java:doWork:481 - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-03-20 16:03:39,215 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1203 - Starting services required for active state
2019-03-20 16:03:39,223 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:435 - Starting recovery process for unclosed journal segments...
2019-03-20 16:03:39,271 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:437 - Successfully started new epoch 5
2019-03-20 16:03:39,271 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:277 - Beginning recovery of unclosed segment starting at txid 1362
2019-03-20 16:03:39,290 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:286 - Recovery prepare phase complete. Responses:
10.10.1.5:8485: segmentState { startTxId: 1362 endTxId: 1398 isInProgress: true } lastWriterEpoch: 4 lastCommittedTxId: 1397
10.10.1.3:8485: segmentState { startTxId: 1362 endTxId: 1398 isInProgress: true } lastWriterEpoch: 4 lastCommittedTxId: 1397
10.10.1.2:8485: segmentState { startTxId: 1362 endTxId: 1398 isInProgress: true } lastWriterEpoch: 4 lastCommittedTxId: 1397
2019-03-20 16:03:39,292 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:310 - Using longest log: 10.10.1.5:8485=segmentState {
  startTxId: 1362
  endTxId: 1398
  isInProgress: true
}
lastWriterEpoch: 4
lastCommittedTxId: 1397

2019-03-20 16:03:39,333 INFO  namenode.FileJournalManager FileJournalManager.java:recoverUnfinalizedSegments:396 - Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-03-20 16:03:39,362 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001199 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001199-0000000000000001361
2019-03-20 16:03:39,378 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1214 - Catching up to latest edits from old active before taking over writer role in edits logs
2019-03-20 16:03:39,383 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@957c646 expecting start txid #1362
2019-03-20 16:03:39,383 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:03:39,384 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1362
2019-03-20 16:03:39,384 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1362
2019-03-20 16:03:39,394 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 37 loaded in 0 seconds
2019-03-20 16:03:39,394 INFO  blockmanagement.DatanodeManager DatanodeManager.java:markAllDatanodesStale:1837 - Marking all datanodes as stale
2019-03-20 16:03:39,395 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1225 - Reprocessing replication and invalidation queues
2019-03-20 16:03:39,395 INFO  blockmanagement.BlockManager BlockManager.java:initializeReplQueues:4752 - initializing replication queues
2019-03-20 16:03:39,396 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1236 - Will take over writing edit logs at txnid 1399
2019-03-20 16:03:39,400 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 1399
2019-03-20 16:03:39,720 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:767 - Initializing quota with 4 thread(s)
2019-03-20 16:03:39,723 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:776 - Quota initialization completed in 4 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-03-20 16:03:39,729 INFO  blockmanagement.CacheReplicationMonitor CacheReplicationMonitor.java:run:160 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-03-20 16:03:39,729 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3447 - Total number of blocks            = 0
2019-03-20 16:03:39,730 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3448 - Number of invalid blocks          = 0
2019-03-20 16:03:39,730 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3449 - Number of under-replicated blocks = 0
2019-03-20 16:03:39,730 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3450 - Number of  over-replicated blocks = 0
2019-03-20 16:03:39,730 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3452 - Number of blocks being written    = 0
2019-03-20 16:03:39,730 INFO  hdfs.StateChange BlockManager.java:processMisReplicatesAsync:3455 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 334 msec
2019-03-20 16:03:39,891 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742133_1309, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:03:39,895 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742134_1310, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:03:40,053 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742135_1311, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:03:40,069 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742136_1312, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:03:40,117 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:03:40,118 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:03:40,118 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:03:40,118 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742137_1313, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:03:40,134 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:03:40,134 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:03:40,135 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:03:40,135 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742138_1314, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:03:40,180 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:03:40,180 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:03:40,180 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:03:40,181 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742139_1315, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:03:40,197 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:03:40,197 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:03:40,197 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:03:40,198 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742140_1316, replicas=10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:03:44,239 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742141_1317, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:03:44,392 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742142_1318, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:03:44,457 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:03:44,457 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:03:44,458 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:03:44,458 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742143_1319, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:03:44,520 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:03:44,520 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:03:44,520 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:03:44,521 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742144_1320, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:04:21,295 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 56 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 1419 Number of syncs: 34 SyncTimes(ms): 299 365 
2019-03-20 16:04:21,353 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742145_1321, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:04:21,507 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742146_1322, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:04:21,572 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:21,572 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:21,572 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:21,572 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742147_1323, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:04:21,618 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:21,618 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:21,618 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:21,618 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742148_1324, replicas=10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:04:22,988 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742149_1325, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:04:23,147 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742150_1326, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:04:23,213 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:23,213 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:23,213 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:23,214 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742151_1327, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:04:23,258 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:23,259 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:23,259 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:23,259 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742152_1328, replicas=10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:04:24,033 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742153_1329, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:04:24,050 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742154_1330, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:04:24,188 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742155_1331, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:04:24,216 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742156_1332, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:04:24,262 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:24,263 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:24,263 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:24,263 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742157_1333, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:04:24,280 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:24,281 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:24,281 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:24,281 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742158_1334, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:04:24,325 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:24,325 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:24,326 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:24,326 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742159_1335, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:04:24,341 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:24,342 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:24,342 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:24,342 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742160_1336, replicas=10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:04:28,344 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742161_1337, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:04:28,502 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742162_1338, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:04:28,561 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:28,561 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:28,561 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:28,562 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742163_1339, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:04:28,615 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:04:28,615 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:04:28,615 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:04:28,616 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742164_1340, replicas=10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:05:05,241 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742165_1341, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:05:05,401 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742166_1342, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:05:05,458 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:05,458 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:05,458 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:05,459 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742167_1343, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:05:05,504 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:05,504 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:05,504 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:05,505 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742168_1344, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:05:06,663 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742169_1345, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:05:06,825 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742170_1346, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:05:06,891 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:06,891 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:06,891 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:06,891 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742171_1347, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:05:06,945 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:06,945 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:06,945 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:06,946 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742172_1348, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:05:07,651 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742173_1349, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:05:07,811 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742174_1350, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:05:07,874 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:07,875 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:07,875 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:07,875 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742175_1351, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:05:07,928 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:07,928 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:07,928 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:07,929 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742176_1352, replicas=10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:05:08,081 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742177_1353, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:05:08,241 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742178_1354, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:05:08,309 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:08,309 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:08,309 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:08,309 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742179_1355, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:05:08,361 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:08,361 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:08,362 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:08,362 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742180_1356, replicas=10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:05:12,069 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742181_1357, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:05:12,223 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742182_1358, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:05:12,288 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:12,288 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:12,288 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:12,289 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742183_1359, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:05:12,334 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:12,334 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:12,334 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:12,335 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742184_1360, replicas=10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:05:49,129 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 236 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 1466 Number of syncs: 167 SyncTimes(ms): 1459 1278 
2019-03-20 16:05:49,192 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742185_1361, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:05:49,354 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742186_1362, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:05:49,419 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:49,419 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:49,420 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:49,420 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742187_1363, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:05:49,466 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:49,466 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:49,466 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:49,466 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742188_1364, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:05:49,480 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 16:05:49,481 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 16:05:49,481 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 1399, 1649
2019-03-20 16:05:49,513 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 252 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 1470 Number of syncs: 180 SyncTimes(ms): 1577 1371 
2019-03-20 16:05:49,517 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001399 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001399-0000000000000001650
2019-03-20 16:05:49,518 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 1651
2019-03-20 16:05:50,374 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742189_1365, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:05:50,542 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742190_1366, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:05:50,607 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:50,607 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:50,607 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:50,608 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742191_1367, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:05:50,669 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:50,669 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:50,670 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:50,670 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742192_1368, replicas=10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:05:51,233 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742193_1369, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:05:51,398 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742194_1370, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:05:51,465 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:51,465 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:51,465 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:51,466 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742195_1371, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:05:51,527 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:51,527 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:51,527 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:51,528 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742196_1372, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:05:51,869 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742197_1373, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:05:52,023 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742198_1374, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:05:52,089 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:52,089 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:52,090 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:52,090 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742199_1375, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:05:52,151 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:52,152 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:52,152 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:52,152 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742200_1376, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:05:55,862 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742201_1377, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:05:56,021 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742202_1378, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:05:56,087 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:56,087 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:56,087 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:56,087 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742203_1379, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:05:56,149 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:05:56,150 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:05:56,150 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:05:56,150 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742204_1380, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:06:33,133 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742205_1381, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:06:33,295 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742206_1382, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:06:33,360 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:33,360 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:33,360 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:33,361 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742207_1383, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:06:33,423 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:33,423 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:33,423 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:33,424 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742208_1384, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:06:34,189 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742209_1385, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:06:34,349 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742210_1386, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:06:34,409 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:34,409 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:34,409 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:34,409 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742211_1387, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:06:34,461 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:34,461 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:34,461 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:34,462 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742212_1388, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:06:34,998 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742213_1389, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:06:35,176 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742214_1390, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:06:35,240 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:35,240 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:35,240 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:35,241 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742215_1391, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:06:35,294 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:35,294 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:35,294 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:35,294 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742216_1392, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:06:35,773 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742217_1393, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:06:35,932 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742218_1394, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:06:35,989 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:35,989 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:35,989 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:35,990 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742219_1395, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:06:36,035 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:36,035 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:36,036 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:36,036 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742220_1396, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:06:39,909 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742221_1397, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:06:40,063 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742222_1398, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:06:40,129 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:40,129 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:40,129 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:40,130 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742223_1399, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:06:40,183 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:06:40,184 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:06:40,184 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:06:40,184 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742224_1400, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:07:16,900 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 166 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 36 Number of syncs: 129 SyncTimes(ms): 984 1124 
2019-03-20 16:07:16,965 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742225_1401, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:07:17,111 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742226_1402, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:07:17,168 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:17,168 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:17,169 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:17,169 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742227_1403, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:07:17,214 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:17,215 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:17,215 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:17,215 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742228_1404, replicas=10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:07:17,907 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742229_1405, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:07:18,061 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742230_1406, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:07:18,129 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:18,129 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:18,129 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:18,130 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742231_1407, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:07:18,181 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:18,181 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:18,182 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:18,182 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742232_1408, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:07:18,708 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742233_1409, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:07:18,869 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742234_1410, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:07:18,935 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:18,935 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:18,935 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:18,935 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742235_1411, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:07:18,989 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:18,990 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:18,990 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:18,990 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742236_1412, replicas=10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:07:19,480 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742237_1413, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:07:19,635 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742238_1414, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:07:19,702 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:19,702 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:19,702 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:19,703 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742239_1415, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:07:19,755 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:19,755 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:19,755 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:19,756 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742240_1416, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:07:23,648 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742241_1417, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:07:23,800 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742242_1418, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:07:23,874 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:23,874 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:23,874 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:23,874 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742243_1419, replicas=10.10.1.6:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:07:23,920 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:07:23,920 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:07:23,920 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:07:23,921 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742244_1420, replicas=10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:07:49,717 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 16:07:49,717 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 16:07:49,717 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 1651, 1905
2019-03-20 16:07:49,738 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 256 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 56 Number of syncs: 200 SyncTimes(ms): 1561 1612 
2019-03-20 16:07:49,741 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001651 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001651-0000000000000001906
2019-03-20 16:07:49,741 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 1907
2019-03-20 16:08:00,743 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742245_1421, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:08:00,899 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742246_1422, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:08:00,957 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:00,957 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:00,958 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:00,958 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742247_1423, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:08:01,011 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:01,011 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:01,011 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:01,011 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742248_1424, replicas=10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:08:01,722 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742249_1425, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:08:01,874 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742250_1426, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:08:01,924 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:01,924 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:01,924 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:01,924 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742251_1427, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:08:01,977 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:01,978 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:01,978 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:01,978 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742252_1428, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:08:02,497 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742253_1429, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:08:02,649 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742254_1430, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:08:02,706 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:02,706 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:02,706 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:02,706 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742255_1431, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:08:02,752 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:02,752 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:02,752 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:02,753 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742256_1432, replicas=10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:08:03,305 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742257_1433, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:08:03,465 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742258_1434, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:08:03,524 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:03,524 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:03,524 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:03,524 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742259_1435, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:08:03,576 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:03,576 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:03,577 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:03,577 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742260_1436, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:08:07,647 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742261_1437, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:08:07,813 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742262_1438, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:08:07,871 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:07,871 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:07,871 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:07,871 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742263_1439, replicas=10.10.1.6:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:08:07,925 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:07,925 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:07,925 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:07,925 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742264_1440, replicas=10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:08:44,549 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742265_1441, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:08:44,711 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742266_1442, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:08:44,768 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:44,768 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:44,768 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:44,768 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742267_1443, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:08:44,814 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:44,814 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:44,814 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:44,815 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742268_1444, replicas=10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:08:45,479 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742269_1445, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:08:45,644 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742270_1446, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:08:45,711 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:45,711 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:45,711 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:45,711 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742271_1447, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:08:45,763 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:45,764 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:45,764 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:45,764 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742272_1448, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:08:46,276 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742273_1449, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:08:46,427 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742274_1450, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:08:46,493 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:46,493 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:46,494 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:46,494 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742275_1451, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:08:46,547 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:46,548 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:46,548 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:46,548 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742276_1452, replicas=10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:08:47,014 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742277_1453, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:08:47,180 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742278_1454, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:08:47,244 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:47,244 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:47,244 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:47,245 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742279_1455, replicas=10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:08:47,297 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:47,297 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:47,297 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:47,297 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742280_1456, replicas=10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:08:51,440 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 164 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 37 Number of syncs: 126 SyncTimes(ms): 1000 951 
2019-03-20 16:08:51,499 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742281_1457, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:08:51,674 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742282_1458, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:08:51,739 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:51,740 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:51,740 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:51,740 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742283_1459, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:08:51,786 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:08:51,786 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:08:51,786 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:08:51,786 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742284_1460, replicas=10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:08:56,311 ERROR namenode.NameNode LogAdapter.java:error:75 - RECEIVED SIGNAL 15: SIGTERM
2019-03-20 16:08:56,313 INFO  namenode.NameNode LogAdapter.java:info:51 - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode059.clemson.cloudlab.us/130.127.133.68
************************************************************/
2019-03-20 16:09:00,437 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-20 16:09:00,449 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-20 16:09:00,454 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-20 16:09:00,699 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-20 16:09:00,744 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-20 16:09:00,851 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-20 16:09:00,851 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-20 16:09:00,902 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-20 16:09:00,902 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-20 16:09:01,066 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-20 16:09:01,094 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-20 16:09:01,112 INFO  util.log Log.java:initialized:192 - Logging initialized @1190ms
2019-03-20 16:09:01,220 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-20 16:09:01,233 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-20 16:09:01,244 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-20 16:09:01,247 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-20 16:09:01,248 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-20 16:09:01,249 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-20 16:09:01,275 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-20 16:09:01,275 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-20 16:09:01,285 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-20 16:09:01,286 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-20 16:09:01,321 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-20 16:09:01,322 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-20 16:09:01,398 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-20 16:09:01,415 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-20 16:09:01,415 INFO  server.Server Server.java:doStart:414 - Started @1495ms
2019-03-20 16:09:01,771 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-20 16:09:01,829 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-20 16:09:01,844 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-20 16:09:01,845 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-20 16:09:01,848 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-20 16:09:01,855 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-20 16:09:01,855 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-20 16:09:01,855 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-20 16:09:01,856 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-20 16:09:01,856 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-20 16:09:01,899 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-20 16:09:01,911 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-20 16:09:01,912 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-20 16:09:01,916 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-20 16:09:01,917 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 20 16:09:01
2019-03-20 16:09:01,919 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-20 16:09:01,919 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:09:01,921 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-20 16:09:01,921 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-20 16:09:02,053 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-20 16:09:02,062 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-20 16:09:02,063 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-20 16:09:02,063 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-20 16:09:02,063 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-20 16:09:02,063 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-20 16:09:02,063 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-20 16:09:02,063 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-20 16:09:02,063 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-20 16:09:02,064 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-20 16:09:02,064 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = true
2019-03-20 16:09:02,064 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-20 16:09:02,148 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-20 16:09:02,148 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:09:02,148 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-20 16:09:02,149 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-20 16:09:02,222 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-20 16:09:02,222 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-20 16:09:02,222 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-20 16:09:02,222 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-20 16:09:02,230 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-20 16:09:02,232 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-20 16:09:02,238 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-20 16:09:02,238 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:09:02,238 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-20 16:09:02,238 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-20 16:09:02,266 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-20 16:09:02,266 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-20 16:09:02,267 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-20 16:09:02,271 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-20 16:09:02,271 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-20 16:09:02,273 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-20 16:09:02,274 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:09:02,274 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-20 16:09:02,274 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-20 16:09:02,323 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 122731@clnode059.clemson.cloudlab.us
2019-03-20 16:09:03,884 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:205 - Detected pause in JVM or host machine (eg GC): pause of approximately 1312ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=359ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1073ms
2019-03-20 16:09:04,051 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-20 16:09:04,122 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-20 16:09:04,124 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-20 16:09:04,155 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-20 16:09:04,155 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-20 16:09:04,161 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@68b6f0d6 expecting start txid #1
2019-03-20 16:09:04,161 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,165 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,165 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,270 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12611 edits # 272 loaded in 0 seconds
2019-03-20 16:09:04,270 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4044fb95 expecting start txid #273
2019-03-20 16:09:04,270 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,271 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,271 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,291 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12613 edits # 272 loaded in 0 seconds
2019-03-20 16:09:04,291 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #545
2019-03-20 16:09:04,292 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,292 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,292 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,302 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 16:09:04,302 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@36f48b4 expecting start txid #636
2019-03-20 16:09:04,302 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,302 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,302 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,314 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 16:09:04,314 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5c00384f expecting start txid #727
2019-03-20 16:09:04,314 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,314 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,315 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,334 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12612 edits # 272 loaded in 0 seconds
2019-03-20 16:09:04,335 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3b7ff809 expecting start txid #999
2019-03-20 16:09:04,335 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,335 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,335 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,347 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 9258 edits # 200 loaded in 0 seconds
2019-03-20 16:09:04,347 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1bb564e2 expecting start txid #1199
2019-03-20 16:09:04,348 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,348 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,348 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,359 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 163 loaded in 0 seconds
2019-03-20 16:09:04,359 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@62e6b5c8 expecting start txid #1362
2019-03-20 16:09:04,359 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,359 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,359 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,364 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 37 loaded in 0 seconds
2019-03-20 16:09:04,365 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3f792b9b expecting start txid #1399
2019-03-20 16:09:04,365 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,365 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,365 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,378 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 11665 edits # 252 loaded in 0 seconds
2019-03-20 16:09:04,378 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7b8233cd expecting start txid #1651
2019-03-20 16:09:04,378 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,379 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,379 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,390 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 11886 edits # 256 loaded in 0 seconds
2019-03-20 16:09:04,390 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4b20ca2b expecting start txid #1907
2019-03-20 16:09:04,390 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:04,391 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,391 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:09:04,400 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 181 loaded in 0 seconds
2019-03-20 16:09:04,401 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-20 16:09:04,401 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 5 entries 115 lookups
2019-03-20 16:09:04,401 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 2119 msecs
2019-03-20 16:09:04,584 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-20 16:09:04,589 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-20 16:09:04,603 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-20 16:09:04,795 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-20 16:09:04,805 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 0
2019-03-20 16:09:04,816 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 0 secs
2019-03-20 16:09:04,816 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 0 racks and 0 datanodes
2019-03-20 16:09:04,816 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-20 16:09:04,847 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-20 16:09:04,847 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-20 16:09:04,853 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-20 16:09:04,856 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-20 16:09:04,863 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-20 16:09:04,869 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-20 16:09:05,063 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 16:09:05,064 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.6:9866
2019-03-20 16:09:05,065 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 4ea34f12-a71f-4771-a263-13486b1812f9 (10.10.1.6:9866).
2019-03-20 16:09:05,067 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 16:09:05,067 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.5:9866
2019-03-20 16:09:05,067 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 42f6c731-38e3-48aa-863c-f30542c9be35 (10.10.1.5:9866).
2019-03-20 16:09:05,068 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 16:09:05,068 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.2:9866
2019-03-20 16:09:05,068 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN cc2f05d0-e01b-494e-bed6-0c53648ffd86 (10.10.1.2:9866).
2019-03-20 16:09:05,068 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 16:09:05,069 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.3:9866
2019-03-20 16:09:05,069 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN e77c7a7a-c22c-4855-abad-bc8542b928ae (10.10.1.3:9866).
2019-03-20 16:09:05,078 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-0832497a-e5df-407b-a464-33936c781e51 for DN 10.10.1.2:9866
2019-03-20 16:09:05,081 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-adde346a-0c8d-4bfe-a820-c69f66587cbd for DN 10.10.1.3:9866
2019-03-20 16:09:05,082 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 for DN 10.10.1.6:9866
2019-03-20 16:09:05,082 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe for DN 10.10.1.5:9866
2019-03-20 16:09:05,093 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xc665bbb918b3febb: Processing first storage report for DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 from datanode 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 16:09:05,095 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xc665bbb918b3febb: from storage DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 node DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2019-03-20 16:09:05,095 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x66a8e2582602776: Processing first storage report for DS-0832497a-e5df-407b-a464-33936c781e51 from datanode cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 16:09:05,095 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x66a8e2582602776: from storage DS-0832497a-e5df-407b-a464-33936c781e51 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:09:05,095 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x58b8c7d6663ea4cf: Processing first storage report for DS-adde346a-0c8d-4bfe-a820-c69f66587cbd from datanode e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 16:09:05,096 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x58b8c7d6663ea4cf: from storage DS-adde346a-0c8d-4bfe-a820-c69f66587cbd node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:09:05,096 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xd67e907b21d8237f: Processing first storage report for DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe from datanode 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 16:09:05,096 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xd67e907b21d8237f: from storage DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:09:23,587 INFO  namenode.FSNamesystem FSNamesystem.java:stopStandbyServices:1408 - Stopping services started for standby state
2019-03-20 16:09:23,588 WARN  ha.EditLogTailer EditLogTailer.java:doWork:481 - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-03-20 16:09:23,591 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1203 - Starting services required for active state
2019-03-20 16:09:23,599 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:435 - Starting recovery process for unclosed journal segments...
2019-03-20 16:09:23,647 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:437 - Successfully started new epoch 7
2019-03-20 16:09:23,647 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:277 - Beginning recovery of unclosed segment starting at txid 2088
2019-03-20 16:09:23,667 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:286 - Recovery prepare phase complete. Responses:
10.10.1.3:8485: segmentState { startTxId: 2088 endTxId: 2088 isInProgress: true } lastWriterEpoch: 6 lastCommittedTxId: 2086
10.10.1.2:8485: segmentState { startTxId: 2088 endTxId: 2088 isInProgress: true } lastWriterEpoch: 6 lastCommittedTxId: 2086
10.10.1.5:8485: segmentState { startTxId: 2088 endTxId: 2088 isInProgress: true } lastWriterEpoch: 6 lastCommittedTxId: 2086
2019-03-20 16:09:23,669 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:310 - Using longest log: 10.10.1.3:8485=segmentState {
  startTxId: 2088
  endTxId: 2088
  isInProgress: true
}
lastWriterEpoch: 6
lastCommittedTxId: 2086

2019-03-20 16:09:23,709 INFO  namenode.FileJournalManager FileJournalManager.java:recoverUnfinalizedSegments:396 - Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-03-20 16:09:23,735 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000001907 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000001907-0000000000000002087
2019-03-20 16:09:23,753 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1214 - Catching up to latest edits from old active before taking over writer role in edits logs
2019-03-20 16:09:23,758 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4f091053 expecting start txid #2088
2019-03-20 16:09:23,758 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:09:23,759 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 2088
2019-03-20 16:09:23,760 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 2088
2019-03-20 16:09:23,765 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-03-20 16:09:23,765 INFO  blockmanagement.DatanodeManager DatanodeManager.java:markAllDatanodesStale:1837 - Marking all datanodes as stale
2019-03-20 16:09:23,766 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1225 - Reprocessing replication and invalidation queues
2019-03-20 16:09:23,766 INFO  blockmanagement.BlockManager BlockManager.java:initializeReplQueues:4752 - initializing replication queues
2019-03-20 16:09:23,767 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1236 - Will take over writing edit logs at txnid 2089
2019-03-20 16:09:23,771 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 2089
2019-03-20 16:09:24,081 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:767 - Initializing quota with 4 thread(s)
2019-03-20 16:09:24,085 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:776 - Quota initialization completed in 3 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-03-20 16:09:24,090 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3447 - Total number of blocks            = 0
2019-03-20 16:09:24,090 INFO  blockmanagement.CacheReplicationMonitor CacheReplicationMonitor.java:run:160 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-03-20 16:09:24,090 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3448 - Number of invalid blocks          = 0
2019-03-20 16:09:24,090 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3449 - Number of under-replicated blocks = 0
2019-03-20 16:09:24,091 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3450 - Number of  over-replicated blocks = 0
2019-03-20 16:09:24,091 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3452 - Number of blocks being written    = 0
2019-03-20 16:09:24,091 INFO  hdfs.StateChange BlockManager.java:processMisReplicatesAsync:3455 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 324 msec
2019-03-20 16:09:32,178 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742285_1461, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:09:32,185 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742286_1462, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:09:32,282 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742287_1463, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:09:32,352 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742288_1464, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:09:32,367 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742289_1465, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:09:32,416 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:32,417 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:32,417 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:32,417 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742290_1466, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:09:32,433 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:32,433 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:32,433 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:32,434 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742291_1467, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:09:32,451 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742292_1468, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:09:32,479 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:32,479 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:32,480 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:32,480 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742293_1469, replicas=10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:09:32,495 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:32,496 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:32,496 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:32,496 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742294_1470, replicas=10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:09:32,516 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:32,517 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:32,517 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:32,517 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742295_1471, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:09:32,579 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:32,580 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:32,580 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:32,580 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742296_1472, replicas=10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:09:32,913 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742297_1473, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:09:33,068 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742298_1474, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:09:33,135 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:33,135 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:33,135 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:33,135 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742299_1475, replicas=10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:09:33,195 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:33,195 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:33,195 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:33,196 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742300_1476, replicas=10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:09:38,910 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742301_1477, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:09:39,072 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742302_1478, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:09:39,138 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:39,139 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:39,139 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:39,139 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742303_1479, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:09:39,200 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:09:39,201 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:09:39,201 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:09:39,201 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742304_1480, replicas=10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:10:16,131 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 92 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 2124 Number of syncs: 55 SyncTimes(ms): 460 556 
2019-03-20 16:10:16,194 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742305_1481, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:10:16,388 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742306_1482, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:10:16,455 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:16,455 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:16,455 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:16,456 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742307_1483, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:10:16,517 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:16,517 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:16,517 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:16,518 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742308_1484, replicas=10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:10:16,590 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742309_1485, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:10:16,733 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742310_1486, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:10:16,771 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742311_1487, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:10:16,836 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:16,836 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:16,836 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:16,836 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742312_1488, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:10:16,886 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742313_1489, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:10:16,887 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742314_1490, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:10:16,898 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:16,899 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:16,899 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:16,899 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742315_1491, replicas=10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:10:16,953 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:16,953 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:16,953 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:16,954 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742316_1492, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:10:17,015 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:17,015 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:17,015 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:17,016 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742317_1493, replicas=10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:10:17,054 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742318_1494, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:10:17,128 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:17,128 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:17,128 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:17,128 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742319_1495, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:10:17,190 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:17,190 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:17,190 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:17,191 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742320_1496, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:10:22,655 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742321_1497, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:10:22,809 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742322_1498, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:10:22,875 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:22,875 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:22,875 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:22,875 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742323_1499, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:10:22,928 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:22,929 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:22,929 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:22,929 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742324_1500, replicas=10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:10:59,643 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742325_1501, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:10:59,826 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742326_1502, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:10:59,889 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:59,890 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:59,890 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:59,890 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742327_1503, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:10:59,952 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:10:59,952 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:10:59,952 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:10:59,952 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742328_1504, replicas=10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:11:00,887 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742329_1505, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:11:01,056 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742330_1506, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:11:01,074 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742331_1507, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:11:01,131 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:01,131 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:01,131 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:01,132 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742332_1508, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:11:01,193 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:01,194 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:01,194 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:01,194 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742333_1509, replicas=10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:11:01,204 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742334_1510, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:11:01,215 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742335_1511, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:11:01,281 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:01,281 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:01,281 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:01,281 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742336_1512, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:11:01,335 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:01,335 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:01,336 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:01,336 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742337_1513, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:11:01,357 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742338_1514, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:11:01,422 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:01,422 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:01,422 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:01,422 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742339_1515, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:11:01,468 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:01,468 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:01,468 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:01,468 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742340_1516, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:11:06,316 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742341_1517, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:11:06,479 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742342_1518, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:11:06,544 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:06,544 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:06,544 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:06,545 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742343_1519, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:11:06,598 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:06,599 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:06,599 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:06,599 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742344_1520, replicas=10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:11:30,160 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 16:11:30,161 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 16:11:30,161 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 2089, 2359
2019-03-20 16:11:30,161 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 272 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 2186 Number of syncs: 173 SyncTimes(ms): 1396 1549 
2019-03-20 16:11:30,201 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 272 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 2186 Number of syncs: 174 SyncTimes(ms): 1428 1558 
2019-03-20 16:11:30,206 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000002089 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000002089-0000000000000002360
2019-03-20 16:11:30,206 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 2361
2019-03-20 16:11:43,363 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742345_1521, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:11:43,507 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742346_1522, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:11:43,547 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:43,547 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:43,548 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:43,548 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742347_1523, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:11:43,585 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:43,585 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:43,585 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:43,586 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742348_1524, replicas=10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:11:45,310 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742349_1525, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:11:45,329 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742350_1526, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:11:45,344 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742351_1527, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:11:45,456 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742352_1528, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:11:45,473 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742353_1529, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:11:45,496 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:45,497 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:45,497 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:45,497 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742354_1530, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:11:45,498 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742355_1531, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:11:45,505 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:45,505 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:45,505 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:45,506 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742356_1532, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:11:45,540 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:45,540 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:45,540 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:45,540 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742357_1533, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:11:45,543 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:45,543 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:45,543 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:45,543 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:45,543 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:45,543 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742358_1534, replicas=10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:11:45,544 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:45,544 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742359_1535, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:11:45,568 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:45,568 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:45,568 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:45,568 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742360_1536, replicas=10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:11:50,177 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742361_1537, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:11:50,322 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742362_1538, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:11:50,361 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:50,362 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:50,362 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:50,362 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742363_1539, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:11:50,399 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:11:50,399 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:11:50,400 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:11:50,400 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742364_1540, replicas=10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:12:27,232 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742365_1541, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:12:27,378 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742366_1542, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:12:27,418 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:27,418 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:27,418 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:27,418 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742367_1543, replicas=10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:12:27,455 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:27,455 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:27,455 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:27,455 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742368_1544, replicas=10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:12:29,035 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742369_1545, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:12:29,064 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742370_1546, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:12:29,176 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742371_1547, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:12:29,201 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742372_1548, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:12:29,226 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:29,226 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:29,226 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:29,226 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742373_1549, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:12:29,250 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:29,250 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:29,250 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:29,250 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742374_1550, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:12:29,263 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:29,263 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:29,263 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:29,263 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742375_1551, replicas=10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:12:29,287 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:29,288 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:29,288 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:29,288 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742376_1552, replicas=10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:12:29,574 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742377_1553, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:12:29,725 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742378_1554, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:12:29,766 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:29,767 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:29,767 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:29,767 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742379_1555, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:12:29,803 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:29,804 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:29,804 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:29,804 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742380_1556, replicas=10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:12:33,815 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 164 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 58 Number of syncs: 105 SyncTimes(ms): 664 522 
2019-03-20 16:12:33,879 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742381_1557, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:12:34,023 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742382_1558, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:12:34,064 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:34,064 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:34,064 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:34,065 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742383_1559, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:12:34,101 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:12:34,102 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:12:34,102 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:12:34,102 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742384_1560, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:13:11,018 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742385_1561, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:13:11,172 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742386_1562, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:13:11,212 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:11,212 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:11,212 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:11,213 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742387_1563, replicas=10.10.1.6:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:13:11,250 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:11,250 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:11,250 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:11,251 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742388_1564, replicas=10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:13:12,878 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742389_1565, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:13:12,897 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742390_1566, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:13:13,021 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742391_1567, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:13:13,029 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742392_1568, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:13:13,053 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:13,053 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:13,053 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:13,053 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742393_1569, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:13:13,061 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:13,061 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:13,062 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:13,062 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742394_1570, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:13:13,082 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:13,083 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:13,083 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:13,083 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742395_1571, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:13:13,090 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:13,091 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:13,091 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:13,091 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742396_1572, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:13:13,476 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742397_1573, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:13:13,612 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742398_1574, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:13:13,652 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:13,653 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:13,653 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:13,653 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742399_1575, replicas=10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:13:13,690 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:13,690 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:13,691 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:13,691 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742400_1576, replicas=10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:13:17,742 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742401_1577, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:13:17,896 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742402_1578, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:13:17,943 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:17,943 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:17,943 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:17,943 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742403_1579, replicas=10.10.1.3:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:13:17,980 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:17,981 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:17,981 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:17,981 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742404_1580, replicas=10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:13:30,412 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 16:13:30,412 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 16:13:30,413 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 2361, 2631
2019-03-20 16:13:30,438 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 272 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 89 Number of syncs: 183 SyncTimes(ms): 1108 902 
2019-03-20 16:13:30,441 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000002361 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000002361-0000000000000002632
2019-03-20 16:13:30,441 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 2633
2019-03-20 16:13:54,847 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742405_1581, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:13:55,000 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742406_1582, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:13:55,041 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:55,041 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:55,041 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:55,041 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742407_1583, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:13:55,079 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:55,079 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:55,079 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:55,080 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742408_1584, replicas=10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:13:56,480 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742409_1585, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:13:56,643 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742410_1586, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:13:56,690 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:56,690 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:56,690 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:56,690 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742411_1587, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:13:56,728 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:56,728 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:56,728 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:56,728 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742412_1588, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:13:56,772 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742413_1589, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:13:56,908 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742414_1590, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:13:56,953 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:56,953 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:56,953 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:56,953 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742415_1591, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:13:56,994 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:56,994 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:56,995 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:56,995 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742416_1592, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:13:57,178 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742417_1593, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:13:57,316 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742418_1594, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:13:57,357 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:57,357 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:57,357 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:57,357 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742419_1595, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:13:57,394 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:13:57,394 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:13:57,394 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:13:57,395 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742420_1596, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:14:01,500 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742421_1597, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:14:01,647 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742422_1598, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:14:01,687 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:14:01,688 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:14:01,688 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:14:01,688 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742423_1599, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:14:01,725 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:14:01,725 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:14:01,726 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:14:01,726 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742424_1600, replicas=10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:14:37,311 ERROR namenode.NameNode LogAdapter.java:error:75 - RECEIVED SIGNAL 15: SIGTERM
2019-03-20 16:14:37,313 INFO  namenode.NameNode LogAdapter.java:info:51 - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at clnode059.clemson.cloudlab.us/130.127.133.68
************************************************************/
2019-03-20 16:14:41,437 INFO  namenode.NameNode LogAdapter.java:info:51 - STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = clnode059.clemson.cloudlab.us/130.127.133.68
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.1
STARTUP_MSG:   classpath = /root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/etc/hadoop:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jsp-api-2.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-common-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/common/hadoop-kms-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-auth-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/hadoop-annotations-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/xz-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-client-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1-tests.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/javax.inject-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/guice-4.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/fst-2.50.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-router-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-core-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-services-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-api-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-registry-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.1.jar:/root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.1.jar
STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'root' on 2019-03-17T18:59Z
STARTUP_MSG:   java = 1.8.0_201
************************************************************/
2019-03-20 16:14:41,449 INFO  namenode.NameNode LogAdapter.java:info:51 - registered UNIX signal handlers for [TERM, HUP, INT]
2019-03-20 16:14:41,454 INFO  namenode.NameNode NameNode.java:createNameNode:1583 - createNameNode []
2019-03-20 16:14:41,697 INFO  beanutils.FluentPropertyBeanIntrospector FluentPropertyBeanIntrospector.java:introspect:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2019-03-20 16:14:41,742 INFO  impl.MetricsConfig MetricsConfig.java:loadFirst:121 - loaded properties from hadoop-metrics2.properties
2019-03-20 16:14:41,847 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:startTimer:374 - Scheduled Metric snapshot period at 10 second(s).
2019-03-20 16:14:41,847 INFO  impl.MetricsSystemImpl MetricsSystemImpl.java:start:191 - NameNode metrics system started
2019-03-20 16:14:41,899 INFO  namenode.NameNodeUtils NameNodeUtils.java:getClientNamenodeAddress:79 - fs.defaultFS is hdfs://mycluster
2019-03-20 16:14:41,899 INFO  namenode.NameNode NameNode.java:<init>:928 - Clients should use mycluster to access this namenode/service.
2019-03-20 16:14:42,067 INFO  util.JvmPauseMonitor JvmPauseMonitor.java:run:188 - Starting JVM pause monitor
2019-03-20 16:14:42,094 INFO  hdfs.DFSUtil DFSUtil.java:httpServerTemplateForNNAndJN:1605 - Starting Web-server for hdfs at: http://node-0-link-0:9870
2019-03-20 16:14:42,112 INFO  util.log Log.java:initialized:192 - Logging initialized @1193ms
2019-03-20 16:14:42,220 INFO  server.AuthenticationFilter AuthenticationFilter.java:constructSecretProvider:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-03-20 16:14:42,233 INFO  http.HttpRequestLog HttpRequestLog.java:getRequestLog:81 - Http request log for http.requests.namenode is not defined
2019-03-20 16:14:42,243 INFO  http.HttpServer2 HttpServer2.java:addGlobalFilter:968 - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-03-20 16:14:42,246 INFO  http.HttpServer2 HttpServer2.java:addFilter:941 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2019-03-20 16:14:42,248 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-03-20 16:14:42,248 INFO  http.HttpServer2 HttpServer2.java:addFilter:951 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-03-20 16:14:42,275 INFO  http.HttpServer2 NameNodeHttpServer.java:initWebHdfs:100 - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2019-03-20 16:14:42,275 INFO  http.HttpServer2 HttpServer2.java:addJerseyResourcePackage:787 - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2019-03-20 16:14:42,284 INFO  http.HttpServer2 HttpServer2.java:bindListener:1185 - Jetty bound to port 9870
2019-03-20 16:14:42,286 INFO  server.Server Server.java:doStart:346 - jetty-9.3.19.v20170502
2019-03-20 16:14:42,321 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@221a3fa4{/logs,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/logs/,AVAILABLE}
2019-03-20 16:14:42,322 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.s.ServletContextHandler@2b40ff9c{/static,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}
2019-03-20 16:14:42,397 INFO  handler.ContextHandler ContextHandler.java:doStart:781 - Started o.e.j.w.WebAppContext@6c7a164b{/,file:///root/hadoop-3.1.1-src/hadoop-dist/target/hadoop-3.1.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}
2019-03-20 16:14:42,415 INFO  server.AbstractConnector AbstractConnector.java:doStart:278 - Started ServerConnector@1cf56a1c{HTTP/1.1,[http/1.1]}{node-0-link-0:9870}
2019-03-20 16:14:42,415 INFO  server.Server Server.java:doStart:414 - Started @1498ms
2019-03-20 16:14:42,777 WARN  namenode.FSNamesystem FSNamesystem.java:checkConfiguration:680 - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2019-03-20 16:14:42,839 INFO  namenode.FSEditLog FSEditLog.java:newInstance:227 - Edit logging is async:true
2019-03-20 16:14:42,853 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:749 - KeyProvider: null
2019-03-20 16:14:42,855 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:122 - fsLock is fair: true
2019-03-20 16:14:42,858 INFO  namenode.FSNamesystem FSNamesystemLock.java:<init>:138 - Detailed lock hold time metrics enabled: false
2019-03-20 16:14:42,864 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:774 - fsOwner             = root (auth:SIMPLE)
2019-03-20 16:14:42,864 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:775 - supergroup          = supergroup
2019-03-20 16:14:42,865 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:776 - isPermissionEnabled = true
2019-03-20 16:14:42,865 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:785 - Determined nameservice ID: mycluster
2019-03-20 16:14:42,865 INFO  namenode.FSNamesystem FSNamesystem.java:<init>:787 - HA Enabled: true
2019-03-20 16:14:42,909 INFO  common.Util Util.java:isDiskStatsEnabled:395 - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-03-20 16:14:42,921 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:301 - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-03-20 16:14:42,922 INFO  blockmanagement.DatanodeManager DatanodeManager.java:<init>:309 - dfs.namenode.datanode.registration.ip-hostname-check=true
2019-03-20 16:14:42,926 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:79 - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-03-20 16:14:42,927 INFO  blockmanagement.BlockManager InvalidateBlocks.java:printBlockDeletionTime:85 - The block deletion will start around 2019 Mar 20 16:14:42
2019-03-20 16:14:42,929 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map BlocksMap
2019-03-20 16:14:42,929 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:14:42,931 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 2.0% max memory 26.7 GB = 546.1 MB
2019-03-20 16:14:42,931 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^26 = 67108864 entries
2019-03-20 16:14:43,067 INFO  blockmanagement.BlockManager BlockManager.java:createBlockTokenSecretManager:579 - dfs.block.access.token.enable = false
2019-03-20 16:14:43,077 INFO  Configuration.deprecation Configuration.java:logDeprecation:1395 - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-03-20 16:14:43,077 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:161 - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-03-20 16:14:43,077 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:162 - dfs.namenode.safemode.min.datanodes = 0
2019-03-20 16:14:43,078 INFO  blockmanagement.BlockManagerSafeMode BlockManagerSafeMode.java:<init>:164 - dfs.namenode.safemode.extension = 30000
2019-03-20 16:14:43,078 INFO  blockmanagement.BlockManager BlockManager.java:<init>:565 - defaultReplication         = 3
2019-03-20 16:14:43,078 INFO  blockmanagement.BlockManager BlockManager.java:<init>:566 - maxReplication             = 512
2019-03-20 16:14:43,078 INFO  blockmanagement.BlockManager BlockManager.java:<init>:567 - minReplication             = 1
2019-03-20 16:14:43,078 INFO  blockmanagement.BlockManager BlockManager.java:<init>:568 - maxReplicationStreams      = 2
2019-03-20 16:14:43,078 INFO  blockmanagement.BlockManager BlockManager.java:<init>:569 - redundancyRecheckInterval  = 3000ms
2019-03-20 16:14:43,078 INFO  blockmanagement.BlockManager BlockManager.java:<init>:570 - encryptDataTransfer        = true
2019-03-20 16:14:43,079 INFO  blockmanagement.BlockManager BlockManager.java:<init>:571 - maxNumBlocksToLog          = 1000
2019-03-20 16:14:43,167 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map INodeMap
2019-03-20 16:14:43,167 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:14:43,168 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 1.0% max memory 26.7 GB = 273.0 MB
2019-03-20 16:14:43,168 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^25 = 33554432 entries
2019-03-20 16:14:43,241 INFO  namenode.FSDirectory FSDirectory.java:<init>:283 - ACLs enabled? false
2019-03-20 16:14:43,241 INFO  namenode.FSDirectory FSDirectory.java:<init>:287 - POSIX ACL inheritance enabled? true
2019-03-20 16:14:43,241 INFO  namenode.FSDirectory FSDirectory.java:<init>:291 - XAttrs enabled? true
2019-03-20 16:14:43,242 INFO  namenode.NameNode FSDirectory.java:<init>:355 - Caching file names occurring more than 10 times
2019-03-20 16:14:43,249 INFO  snapshot.SnapshotManager SnapshotManager.java:<init>:124 - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-03-20 16:14:43,251 INFO  snapshot.SnapshotManager DirectoryDiffListFactory.java:init:43 - SkipList is disabled
2019-03-20 16:14:43,257 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map cachedBlocks
2019-03-20 16:14:43,257 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:14:43,257 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.25% max memory 26.7 GB = 68.3 MB
2019-03-20 16:14:43,257 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^23 = 8388608 entries
2019-03-20 16:14:43,285 INFO  metrics.TopMetrics TopMetrics.java:logConf:76 - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-03-20 16:14:43,286 INFO  metrics.TopMetrics TopMetrics.java:logConf:78 - NNTop conf: dfs.namenode.top.num.users = 10
2019-03-20 16:14:43,286 INFO  metrics.TopMetrics TopMetrics.java:logConf:80 - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-03-20 16:14:43,290 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:979 - Retry cache on namenode is enabled
2019-03-20 16:14:43,290 INFO  namenode.FSNamesystem FSNamesystem.java:initRetryCache:987 - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-03-20 16:14:43,293 INFO  util.GSet LightWeightGSet.java:computeCapacity:395 - Computing capacity for map NameNodeRetryCache
2019-03-20 16:14:43,293 INFO  util.GSet LightWeightGSet.java:computeCapacity:396 - VM type       = 64-bit
2019-03-20 16:14:43,293 INFO  util.GSet LightWeightGSet.java:computeCapacity:397 - 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-03-20 16:14:43,293 INFO  util.GSet LightWeightGSet.java:computeCapacity:402 - capacity      = 2^20 = 1048576 entries
2019-03-20 16:14:43,343 INFO  common.Storage Storage.java:tryLock:905 - Lock on /tmp/hadoop-root/dfs/name/in_use.lock acquired by nodename 1858@clnode059.clemson.cloudlab.us
2019-03-20 16:14:44,697 INFO  namenode.FSImage FSImage.java:loadFSImageFile:782 - Planning to load image: FSImageFile(file=/tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2019-03-20 16:14:44,759 INFO  namenode.ErasureCodingPolicyManager ErasureCodingPolicyManager.java:enablePolicy:390 - Enable the erasure coding policy RS-6-3-1024k
2019-03-20 16:14:44,762 INFO  namenode.FSImageFormatPBINode FSImageFormatPBINode.java:loadINodeSection:266 - Loading 1 INodes.
2019-03-20 16:14:44,793 INFO  namenode.FSImageFormatProtobuf FSImageFormatProtobuf.java:load:190 - Loaded FSImage in 0 seconds.
2019-03-20 16:14:44,793 INFO  namenode.FSImage FSImage.java:loadFSImage:951 - Loaded image for txid 0 from /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000
2019-03-20 16:14:44,799 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@aa549e5 expecting start txid #1
2019-03-20 16:14:44,799 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:44,803 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,803 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,907 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12611 edits # 272 loaded in 0 seconds
2019-03-20 16:14:44,908 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@36f48b4 expecting start txid #273
2019-03-20 16:14:44,908 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:44,908 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,908 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,929 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=273&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12613 edits # 272 loaded in 0 seconds
2019-03-20 16:14:44,929 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5c00384f expecting start txid #545
2019-03-20 16:14:44,929 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:44,929 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,929 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,940 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=545&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 16:14:44,940 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3b7ff809 expecting start txid #636
2019-03-20 16:14:44,940 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:44,940 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,940 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,950 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=636&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 16:14:44,950 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1bb564e2 expecting start txid #727
2019-03-20 16:14:44,950 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:44,951 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,951 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,968 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=727&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12612 edits # 272 loaded in 0 seconds
2019-03-20 16:14:44,968 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@62e6b5c8 expecting start txid #999
2019-03-20 16:14:44,969 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:44,969 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,969 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,981 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=999&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 9258 edits # 200 loaded in 0 seconds
2019-03-20 16:14:44,982 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3f792b9b expecting start txid #1199
2019-03-20 16:14:44,982 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:44,982 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,982 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,993 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1199&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 163 loaded in 0 seconds
2019-03-20 16:14:44,993 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7b8233cd expecting start txid #1362
2019-03-20 16:14:44,993 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:44,993 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,994 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,998 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1362&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 37 loaded in 0 seconds
2019-03-20 16:14:44,999 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4b20ca2b expecting start txid #1399
2019-03-20 16:14:44,999 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:44,999 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:44,999 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,012 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1399&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 11665 edits # 252 loaded in 0 seconds
2019-03-20 16:14:45,012 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1cbf6e72 expecting start txid #1651
2019-03-20 16:14:45,012 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:45,012 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,012 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,025 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1651&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 11886 edits # 256 loaded in 0 seconds
2019-03-20 16:14:45,025 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6aecbb8d expecting start txid #1907
2019-03-20 16:14:45,025 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:45,025 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,026 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,035 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=1907&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 181 loaded in 0 seconds
2019-03-20 16:14:45,035 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1af146 expecting start txid #2088
2019-03-20 16:14:45,035 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:45,036 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,036 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,039 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2088&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 1 loaded in 0 seconds
2019-03-20 16:14:45,039 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4da602fc expecting start txid #2089
2019-03-20 16:14:45,039 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:45,040 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,040 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,053 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2089&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12617 edits # 272 loaded in 0 seconds
2019-03-20 16:14:45,053 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2a8d39c4 expecting start txid #2361
2019-03-20 16:14:45,053 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:45,053 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,053 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,064 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2361&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 12612 edits # 272 loaded in 0 seconds
2019-03-20 16:14:45,064 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@25b2cfcb expecting start txid #2633
2019-03-20 16:14:45,064 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:14:45,064 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,065 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 1
2019-03-20 16:14:45,071 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-4-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2633&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 16:14:45,071 INFO  namenode.FSNamesystem FSNamesystem.java:loadFSImage:1091 - Need to save fs image? false (staleImage=false, haEnabled=true, isRollingUpgrade=false)
2019-03-20 16:14:45,071 INFO  namenode.NameCache NameCache.java:initialized:143 - initialized with 5 entries 150 lookups
2019-03-20 16:14:45,072 INFO  namenode.FSNamesystem FSNamesystem.java:loadFromDisk:721 - Finished loading FSImage in 1770 msecs
2019-03-20 16:14:45,258 INFO  namenode.NameNode NameNodeRpcServer.java:<init>:446 - RPC server is binding to node-0-link-0:8020
2019-03-20 16:14:45,263 INFO  ipc.CallQueueManager CallQueueManager.java:<init>:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-03-20 16:14:45,276 INFO  ipc.Server Server.java:run:1070 - Starting Socket Reader #1 for port 8020
2019-03-20 16:14:45,472 INFO  namenode.FSNamesystem FSNamesystem.java:registerMBean:4973 - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2019-03-20 16:14:45,483 INFO  namenode.LeaseManager LeaseManager.java:getNumUnderConstructionBlocks:171 - Number of blocks under construction: 0
2019-03-20 16:14:45,495 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:396 - STATE* Leaving safe mode after 0 secs
2019-03-20 16:14:45,495 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:402 - STATE* Network topology has 0 racks and 0 datanodes
2019-03-20 16:14:45,495 INFO  hdfs.StateChange BlockManagerSafeMode.java:leaveSafeMode:404 - STATE* UnderReplicatedBlocks has 0 blocks
2019-03-20 16:14:45,528 INFO  ipc.Server Server.java:run:1310 - IPC Server Responder: starting
2019-03-20 16:14:45,528 INFO  ipc.Server Server.java:run:1149 - IPC Server listener on 8020: starting
2019-03-20 16:14:45,534 INFO  namenode.NameNode NameNode.java:startCommonServices:812 - NameNode RPC up at: node-0-link-0/10.10.1.1:8020
2019-03-20 16:14:45,538 INFO  namenode.FSNamesystem FSNamesystem.java:startStandbyServices:1365 - Starting services required for standby state
2019-03-20 16:14:45,544 INFO  ha.EditLogTailer EditLogTailer.java:<init>:188 - Will roll logs on active node every 120 seconds.
2019-03-20 16:14:45,550 INFO  ha.StandbyCheckpointer StandbyCheckpointer.java:start:139 - Starting standby checkpoint thread...
Checkpointing active NN to possible NNs: [http://node-1-link-0:9870]
Serving checkpoints at http://node-0-link-0:9870
2019-03-20 16:14:46,282 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 16:14:46,283 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.2:9866
2019-03-20 16:14:46,284 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN cc2f05d0-e01b-494e-bed6-0c53648ffd86 (10.10.1.2:9866).
2019-03-20 16:14:46,285 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 16:14:46,286 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.3:9866
2019-03-20 16:14:46,286 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN e77c7a7a-c22c-4855-abad-bc8542b928ae (10.10.1.3:9866).
2019-03-20 16:14:46,286 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 16:14:46,286 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.6:9866
2019-03-20 16:14:46,287 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 4ea34f12-a71f-4771-a263-13486b1812f9 (10.10.1.6:9866).
2019-03-20 16:14:46,287 INFO  hdfs.StateChange DatanodeManager.java:registerDatanode:1040 - BLOCK* registerDatanode: from DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748) storage 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 16:14:46,287 INFO  net.NetworkTopology NetworkTopology.java:add:145 - Adding a new node: /default-rack/10.10.1.5:9866
2019-03-20 16:14:46,287 INFO  blockmanagement.BlockReportLeaseManager BlockReportLeaseManager.java:registerNode:204 - Registered DN 42f6c731-38e3-48aa-863c-f30542c9be35 (10.10.1.5:9866).
2019-03-20 16:14:46,296 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 for DN 10.10.1.6:9866
2019-03-20 16:14:46,298 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe for DN 10.10.1.5:9866
2019-03-20 16:14:46,299 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-adde346a-0c8d-4bfe-a820-c69f66587cbd for DN 10.10.1.3:9866
2019-03-20 16:14:46,299 INFO  blockmanagement.DatanodeDescriptor DatanodeDescriptor.java:updateStorage:987 - Adding new storage ID DS-0832497a-e5df-407b-a464-33936c781e51 for DN 10.10.1.2:9866
2019-03-20 16:14:46,310 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x66a8e2582602777: Processing first storage report for DS-0832497a-e5df-407b-a464-33936c781e51 from datanode cc2f05d0-e01b-494e-bed6-0c53648ffd86
2019-03-20 16:14:46,311 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x66a8e2582602777: from storage DS-0832497a-e5df-407b-a464-33936c781e51 node DatanodeRegistration(10.10.1.2:9866, datanodeUuid=cc2f05d0-e01b-494e-bed6-0c53648ffd86, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:14:46,311 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0x58b8c7d6663ea4d0: Processing first storage report for DS-adde346a-0c8d-4bfe-a820-c69f66587cbd from datanode e77c7a7a-c22c-4855-abad-bc8542b928ae
2019-03-20 16:14:46,312 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0x58b8c7d6663ea4d0: from storage DS-adde346a-0c8d-4bfe-a820-c69f66587cbd node DatanodeRegistration(10.10.1.3:9866, datanodeUuid=e77c7a7a-c22c-4855-abad-bc8542b928ae, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:14:46,312 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xd67e907b21d82380: Processing first storage report for DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe from datanode 42f6c731-38e3-48aa-863c-f30542c9be35
2019-03-20 16:14:46,312 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xd67e907b21d82380: from storage DS-8db4f22a-f1f5-4ac9-8505-5d12993c3ffe node DatanodeRegistration(10.10.1.5:9866, datanodeUuid=42f6c731-38e3-48aa-863c-f30542c9be35, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:14:46,312 INFO  BlockStateChange BlockManager.java:processReport:2526 - BLOCK* processReport 0xc665bbb918b3febc: Processing first storage report for DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 from datanode 4ea34f12-a71f-4771-a263-13486b1812f9
2019-03-20 16:14:46,312 INFO  BlockStateChange BlockManager.java:processReport:2555 - BLOCK* processReport 0xc665bbb918b3febc: from storage DS-2dae0a24-0bc7-4d95-88a1-01e93d9991d4 node DatanodeRegistration(10.10.1.6:9866, datanodeUuid=4ea34f12-a71f-4771-a263-13486b1812f9, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-685a9016-70e4-4e31-8348-b59aa1315c6b;nsid=1104414156;c=1553118704748), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2019-03-20 16:15:05,097 INFO  namenode.FSNamesystem FSNamesystem.java:stopStandbyServices:1408 - Stopping services started for standby state
2019-03-20 16:15:05,098 WARN  ha.EditLogTailer EditLogTailer.java:doWork:481 - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:479)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:409)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:426)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:422)
2019-03-20 16:15:05,101 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1203 - Starting services required for active state
2019-03-20 16:15:05,108 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:435 - Starting recovery process for unclosed journal segments...
2019-03-20 16:15:05,156 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnfinalizedSegments:437 - Successfully started new epoch 9
2019-03-20 16:15:05,156 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:277 - Beginning recovery of unclosed segment starting at txid 2724
2019-03-20 16:15:05,176 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:286 - Recovery prepare phase complete. Responses:
10.10.1.3:8485: segmentState { startTxId: 2724 endTxId: 2814 isInProgress: true } lastWriterEpoch: 8 lastCommittedTxId: 2813
10.10.1.5:8485: segmentState { startTxId: 2724 endTxId: 2814 isInProgress: true } lastWriterEpoch: 8 lastCommittedTxId: 2813
10.10.1.2:8485: segmentState { startTxId: 2724 endTxId: 2814 isInProgress: true } lastWriterEpoch: 8 lastCommittedTxId: 2813
2019-03-20 16:15:05,178 INFO  client.QuorumJournalManager QuorumJournalManager.java:recoverUnclosedSegment:310 - Using longest log: 10.10.1.3:8485=segmentState {
  startTxId: 2724
  endTxId: 2814
  isInProgress: true
}
lastWriterEpoch: 8
lastCommittedTxId: 2813

2019-03-20 16:15:05,221 INFO  namenode.FileJournalManager FileJournalManager.java:recoverUnfinalizedSegments:396 - Recovering unfinalized segments in /tmp/hadoop-root/dfs/name/current
2019-03-20 16:15:05,240 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000002633 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000002633-0000000000000002723
2019-03-20 16:15:05,256 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1214 - Catching up to latest edits from old active before taking over writer role in edits logs
2019-03-20 16:15:05,261 INFO  namenode.FSImage FSImage.java:loadEdits:887 - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@494af39a expecting start txid #2724
2019-03-20 16:15:05,261 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:158 - Start loading edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2724&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2724&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true maxTxnsToRead = 9223372036854775807
2019-03-20 16:15:05,262 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2724&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2724&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 2724
2019-03-20 16:15:05,262 INFO  namenode.RedundantEditLogInputStream RedundantEditLogInputStream.java:nextOp:177 - Fast-forwarding stream 'http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2724&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true' to transaction ID 2724
2019-03-20 16:15:05,271 INFO  namenode.FSImage FSEditLogLoader.java:loadFSEdits:162 - Edits file http://node-3-link-0:8480/getJournal?jid=mycluster&segmentTxId=2724&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true, http://node-2-link-0:8480/getJournal?jid=mycluster&segmentTxId=2724&storageInfo=-64%3A1104414156%3A1553118704748%3ACID-685a9016-70e4-4e31-8348-b59aa1315c6b&inProgressOk=true of size 1048576 edits # 91 loaded in 0 seconds
2019-03-20 16:15:05,272 INFO  blockmanagement.DatanodeManager DatanodeManager.java:markAllDatanodesStale:1837 - Marking all datanodes as stale
2019-03-20 16:15:05,273 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1225 - Reprocessing replication and invalidation queues
2019-03-20 16:15:05,273 INFO  blockmanagement.BlockManager BlockManager.java:initializeReplQueues:4752 - initializing replication queues
2019-03-20 16:15:05,273 INFO  namenode.FSNamesystem FSNamesystem.java:startActiveServices:1236 - Will take over writing edit logs at txnid 2815
2019-03-20 16:15:05,276 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 2815
2019-03-20 16:15:05,562 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:767 - Initializing quota with 4 thread(s)
2019-03-20 16:15:05,566 INFO  namenode.FSDirectory FSDirectory.java:updateCountForQuota:776 - Quota initialization completed in 3 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2019-03-20 16:15:05,571 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3447 - Total number of blocks            = 0
2019-03-20 16:15:05,571 INFO  blockmanagement.CacheReplicationMonitor CacheReplicationMonitor.java:run:160 - Starting CacheReplicationMonitor with interval 30000 milliseconds
2019-03-20 16:15:05,571 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3448 - Number of invalid blocks          = 0
2019-03-20 16:15:05,571 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3449 - Number of under-replicated blocks = 0
2019-03-20 16:15:05,571 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3450 - Number of  over-replicated blocks = 0
2019-03-20 16:15:05,572 INFO  blockmanagement.BlockManager BlockManager.java:processMisReplicatesAsync:3452 - Number of blocks being written    = 0
2019-03-20 16:15:05,572 INFO  hdfs.StateChange BlockManager.java:processMisReplicatesAsync:3455 - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 298 msec
2019-03-20 16:15:31,289 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742445_1621, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:15:31,457 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742446_1622, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:15:31,497 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:31,497 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:31,498 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:31,498 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742447_1623, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:15:31,534 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:31,535 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:31,535 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:31,535 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742448_1624, replicas=10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:15:33,395 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742449_1625, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:15:33,548 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742450_1626, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:15:33,589 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:33,589 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:33,589 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:33,589 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742451_1627, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:15:33,625 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:33,626 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:33,626 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:33,626 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742452_1628, replicas=10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:15:33,760 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742453_1629, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:15:33,922 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742454_1630, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:15:33,963 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:33,963 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:33,964 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:33,964 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742455_1631, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:15:34,000 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:34,001 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:34,001 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:34,001 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742456_1632, replicas=10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:15:34,184 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742457_1633, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:15:34,331 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742458_1634, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:15:34,371 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:34,371 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:34,371 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:34,371 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742459_1635, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:15:34,408 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:34,409 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:34,409 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:34,409 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742460_1636, replicas=10.10.1.6:9866 for /myfile1._COPYING_
2019-03-20 16:15:36,418 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742461_1637, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:15:36,563 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742462_1638, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:15:36,603 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:36,604 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:36,604 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:36,604 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742463_1639, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:15:36,640 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:15:36,641 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:15:36,641 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:15:36,641 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742464_1640, replicas=10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:16:14,791 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 92 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 2834 Number of syncs: 71 SyncTimes(ms): 490 337 
2019-03-20 16:16:14,848 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742465_1641, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:16:14,993 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742466_1642, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:16:15,033 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:15,034 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:15,034 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:15,034 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742467_1643, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:16:15,071 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:15,071 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:15,072 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:15,072 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742468_1644, replicas=10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:16:16,773 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742469_1645, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:16:16,943 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742470_1646, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:16:16,983 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:16,983 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:16,983 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:16,983 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742471_1647, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:16:17,021 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:17,021 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:17,021 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:17,021 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742472_1648, replicas=10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:16:17,571 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742473_1649, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:16:17,717 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742474_1650, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:16:17,757 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:17,758 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:17,758 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:17,758 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742475_1651, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:16:17,795 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:17,795 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:17,795 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:17,796 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742476_1652, replicas=10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:16:17,938 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742477_1653, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:16:18,095 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742478_1654, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:16:18,151 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:18,151 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:18,152 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:18,152 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742479_1655, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:16:18,186 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:18,187 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:18,187 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:18,187 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742480_1656, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:16:19,733 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742481_1657, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:16:19,891 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742482_1658, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:16:19,957 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:19,957 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:19,957 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:19,957 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742483_1659, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:16:20,010 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:20,011 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:20,011 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:20,011 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742484_1660, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:16:58,667 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742485_1661, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:16:58,821 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742486_1662, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:16:58,862 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:58,862 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:58,862 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:58,862 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742487_1663, replicas=10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:16:58,899 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:16:58,900 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:16:58,900 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:16:58,900 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742488_1664, replicas=10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:17:00,344 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742489_1665, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:17:00,495 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742490_1666, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:17:00,536 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:00,536 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:00,536 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:00,536 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742491_1667, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:17:00,573 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:00,574 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:00,574 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:00,574 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742492_1668, replicas=10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:17:01,686 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742493_1669, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:17:01,828 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742494_1670, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:17:01,888 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:01,888 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:01,888 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:01,888 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742495_1671, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:17:01,923 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:01,923 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:01,923 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:01,924 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742496_1672, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:17:02,048 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742497_1673, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:17:02,195 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742498_1674, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:17:02,236 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:02,236 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:02,237 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:02,237 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742499_1675, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:17:02,273 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:02,273 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:02,273 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:02,273 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742500_1676, replicas=10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:17:03,781 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742501_1677, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:17:03,927 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742502_1678, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:17:03,968 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:03,968 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:03,968 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:03,968 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742503_1679, replicas=10.10.1.6:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:17:04,005 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:04,005 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:04,006 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:04,006 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742504_1680, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:17:15,159 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 16:17:15,160 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 16:17:15,160 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 2815, 3085
2019-03-20 16:17:15,160 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 272 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 2874 Number of syncs: 211 SyncTimes(ms): 1520 950 
2019-03-20 16:17:15,189 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 272 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 2874 Number of syncs: 212 SyncTimes(ms): 1537 961 
2019-03-20 16:17:15,193 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000002815 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000002815-0000000000000003086
2019-03-20 16:17:15,193 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 3087
2019-03-20 16:17:42,599 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742505_1681, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:17:42,746 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742506_1682, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:17:42,812 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:42,813 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:42,813 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:42,813 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742507_1683, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:17:42,866 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:42,866 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:42,867 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:42,867 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742508_1684, replicas=10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:17:44,346 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742509_1685, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:17:44,504 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742510_1686, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:17:44,570 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:44,571 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:44,571 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:44,571 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742511_1687, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:17:44,624 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:44,624 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:44,624 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:44,624 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742512_1688, replicas=10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:17:45,634 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742513_1689, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:17:45,788 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742514_1690, replicas=10.10.1.2:9866, 10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:17:45,863 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:45,864 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:45,864 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:45,864 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742515_1691, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:17:45,932 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:45,932 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:45,932 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:45,932 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742516_1692, replicas=10.10.1.5:9866 for /myfile2._COPYING_
2019-03-20 16:17:46,075 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742517_1693, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:17:46,237 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742518_1694, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:17:46,302 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:46,302 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:46,302 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:46,303 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742519_1695, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:17:46,356 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:46,356 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:46,357 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:46,357 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742520_1696, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:17:47,530 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742521_1697, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:17:47,703 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742522_1698, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:17:47,768 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:47,768 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:47,769 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:47,769 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742523_1699, replicas=10.10.1.3:9866, 10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:17:47,823 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:17:47,823 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:17:47,823 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:17:47,824 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742524_1700, replicas=10.10.1.5:9866 for /myfile5._COPYING_
2019-03-20 16:18:26,543 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 92 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 20 Number of syncs: 71 SyncTimes(ms): 570 621 
2019-03-20 16:18:26,607 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742525_1701, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:18:26,749 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742526_1702, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:18:26,808 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:26,808 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:26,808 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:26,809 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742527_1703, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:18:26,861 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:26,861 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:26,861 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:26,862 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742528_1704, replicas=10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:18:28,028 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742529_1705, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:18:28,200 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742530_1706, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:18:28,265 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:28,265 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:28,265 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:28,265 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742531_1707, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:18:28,319 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:28,319 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:28,319 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:28,319 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742532_1708, replicas=10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:18:29,545 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742533_1709, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:18:29,698 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742534_1710, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:18:29,764 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:29,764 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:29,764 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:29,765 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742535_1711, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:18:29,818 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:29,818 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:29,818 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:29,818 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742536_1712, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:18:30,019 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742537_1713, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:18:30,198 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742538_1714, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:18:30,263 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:30,264 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:30,264 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:30,264 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742539_1715, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:18:30,318 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:30,318 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:30,318 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:30,318 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742540_1716, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:18:31,388 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742541_1717, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:18:31,562 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742542_1718, replicas=10.10.1.5:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:18:31,646 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:31,646 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:31,646 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:31,647 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742543_1719, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:18:31,700 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:18:31,701 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:18:31,701 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:18:31,701 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742544_1720, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:19:10,691 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742545_1721, replicas=10.10.1.2:9866, 10.10.1.6:9866, 10.10.1.5:9866 for /myfile3._COPYING_
2019-03-20 16:19:10,869 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742546_1722, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:19:10,934 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:10,935 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:10,935 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:10,935 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742547_1723, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:19:10,989 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:10,989 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:10,989 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:10,990 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742548_1724, replicas=10.10.1.6:9866 for /myfile3._COPYING_
2019-03-20 16:19:11,925 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742549_1725, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.3:9866 for /myfile4._COPYING_
2019-03-20 16:19:12,111 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742550_1726, replicas=10.10.1.3:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile4._COPYING_
2019-03-20 16:19:12,176 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:12,176 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:12,176 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:12,176 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742551_1727, replicas=10.10.1.6:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:19:12,238 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:12,239 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:12,239 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:12,239 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742552_1728, replicas=10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:19:13,260 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742553_1729, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:19:13,417 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742554_1730, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile2._COPYING_
2019-03-20 16:19:13,484 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:13,484 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:13,484 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:13,484 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742555_1731, replicas=10.10.1.3:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:19:13,546 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:13,546 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:13,546 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:13,547 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742556_1732, replicas=10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:19:14,089 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742557_1733, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:19:14,259 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742558_1734, replicas=10.10.1.6:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:19:14,326 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:14,326 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:14,326 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:14,326 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742559_1735, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:19:14,387 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:14,387 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:14,387 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:14,388 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742560_1736, replicas=10.10.1.3:9866 for /myfile1._COPYING_
2019-03-20 16:19:15,358 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742561_1737, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:19:15,383 INFO  namenode.FSNamesystem FSNamesystem.java:rollEditLog:4646 - Roll Edit Log from 10.10.1.4
2019-03-20 16:19:15,383 INFO  namenode.FSEditLog FSEditLog.java:rollEditLog:1314 - Rolling edit logs
2019-03-20 16:19:15,383 INFO  namenode.FSEditLog FSEditLog.java:endCurrentLogSegment:1406 - Ending log segment 3087, 3343
2019-03-20 16:19:15,410 INFO  namenode.FSEditLog FSEditLog.java:printStatistics:774 - Number of transactions: 258 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 58 Number of syncs: 200 SyncTimes(ms): 1656 1737 
2019-03-20 16:19:15,412 INFO  namenode.FileJournalManager FileJournalManager.java:finalizeLogSegment:143 - Finalizing edits file /tmp/hadoop-root/dfs/name/current/edits_inprogress_0000000000000003087 -> /tmp/hadoop-root/dfs/name/current/edits_0000000000000003087-0000000000000003344
2019-03-20 16:19:15,412 INFO  namenode.FSEditLog FSEditLog.java:startLogSegment:1361 - Starting log segment at 3345
2019-03-20 16:19:15,578 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742562_1738, replicas=10.10.1.6:9866, 10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:19:15,643 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:15,643 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:15,643 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:15,643 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742563_1739, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:19:15,705 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:15,706 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:15,706 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:15,706 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742564_1740, replicas=10.10.1.3:9866 for /myfile5._COPYING_
2019-03-20 16:19:54,365 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742565_1741, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:19:54,543 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742566_1742, replicas=10.10.1.5:9866, 10.10.1.3:9866, 10.10.1.2:9866 for /myfile3._COPYING_
2019-03-20 16:19:54,607 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:54,607 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:54,607 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:54,607 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742567_1743, replicas=10.10.1.2:9866, 10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:19:54,661 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:54,661 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:54,661 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:54,661 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742568_1744, replicas=10.10.1.3:9866 for /myfile3._COPYING_
2019-03-20 16:19:55,835 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742569_1745, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile4._COPYING_
2019-03-20 16:19:55,990 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742570_1746, replicas=10.10.1.6:9866, 10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:19:56,057 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:56,058 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:56,058 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:56,058 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742571_1747, replicas=10.10.1.5:9866, 10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:19:56,110 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:56,110 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:56,111 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:56,111 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742572_1748, replicas=10.10.1.2:9866 for /myfile4._COPYING_
2019-03-20 16:19:56,812 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742573_1749, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile2._COPYING_
2019-03-20 16:19:56,982 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742574_1750, replicas=10.10.1.2:9866, 10.10.1.5:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:19:57,048 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:57,048 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:57,048 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:57,049 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742575_1751, replicas=10.10.1.5:9866, 10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:19:57,101 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:57,102 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:57,102 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:57,102 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742576_1752, replicas=10.10.1.6:9866 for /myfile2._COPYING_
2019-03-20 16:19:57,986 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742577_1753, replicas=10.10.1.3:9866, 10.10.1.2:9866, 10.10.1.5:9866 for /myfile1._COPYING_
2019-03-20 16:19:58,140 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742578_1754, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:19:58,205 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:58,205 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:58,206 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:58,206 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742579_1755, replicas=10.10.1.6:9866, 10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:19:58,259 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:58,260 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:58,260 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:58,260 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742580_1756, replicas=10.10.1.2:9866 for /myfile1._COPYING_
2019-03-20 16:19:59,047 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742581_1757, replicas=10.10.1.3:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:19:59,206 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742582_1758, replicas=10.10.1.5:9866, 10.10.1.6:9866, 10.10.1.2:9866 for /myfile5._COPYING_
2019-03-20 16:19:59,272 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:59,272 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:59,272 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:59,272 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742583_1759, replicas=10.10.1.2:9866, 10.10.1.6:9866 for /myfile5._COPYING_
2019-03-20 16:19:59,334 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2019-03-20 16:19:59,334 WARN  protocol.BlockStoragePolicy BlockStoragePolicy.java:chooseStorageTypes:161 - Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2019-03-20 16:19:59,334 WARN  blockmanagement.BlockPlacementPolicy BlockPlacementPolicyDefault.java:chooseTarget:427 - Failed to place enough replicas, still in need of 2 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2019-03-20 16:19:59,334 INFO  hdfs.StateChange FSDirWriteFileOp.java:logAllocatedBlock:796 - BLOCK* allocate blk_1073742584_1760, replicas=10.10.1.6:9866 for /myfile5._COPYING_
